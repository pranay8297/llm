{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyM9TvM3guAOQi+hXIa9hY6x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranay8297/llm/blob/main/esm/esm_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "epQUm2Lr5CTp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmsBT-314zrX"
      },
      "outputs": [],
      "source": [
        "! pip install transformers evaluate datasets requests pandas sklearn\n",
        "! pip install datasets\n",
        "! pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from datasets import load_dataset\n",
        "from dataclasses import dataclass\n",
        "from einops import rearrange, repeat\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "PFGPT_VOCAB_SIZE = 384\n",
        "PFGPT_HF_MODEL_PATH = 'lamm-mit/ProteinForceGPT'\n",
        "\n",
        "@dataclass\n",
        "class LoRAConfig:\n",
        "    lora_r: int = 8\n",
        "    lora_alpha: int = 16\n",
        "    lora_dropout: int = 0.05\n",
        "    lora_query: bool = True\n",
        "    lora_key: bool = False\n",
        "    lora_value: bool = True\n",
        "    lora_projection: bool = False\n",
        "    lora_mlp: bool = False\n",
        "    lora_head: bool = False\n",
        "\n",
        "class LoRALinear(nn.Linear):\n",
        "    def __init__(self, nin, nout, lora_config):\n",
        "        super().__init__(nin, nout)\n",
        "        std_dev = 1 / torch.sqrt(torch.tensor(lora_config.lora_r).float())\n",
        "        self.lora_A = torch.nn.Parameter(torch.randn(nin, lora_config.lora_r) * std_dev)\n",
        "        self.lora_B = torch.nn.Parameter(torch.zeros(lora_config.lora_r, nout))\n",
        "        self.alpha = lora_config.lora_alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        lora_x = self.alpha * (x @ self.lora_A @ self.lora_B)\n",
        "        x = super().forward(x)\n",
        "        return x + lora_x\n",
        "\n",
        "def get_tokenizer():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(PFGPT_HF_MODEL_PATH, trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    return tokenizer\n",
        "\n",
        "def rotate_half(x):\n",
        "    x1, x2 = x.chunk(2, dim=-1)\n",
        "    return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "def apply_rotary_pos_emb(x, cos, sin):\n",
        "    cos = cos[:, :, : x.shape[-2], :]\n",
        "    sin = sin[:, :, : x.shape[-2], :]\n",
        "\n",
        "    return (x * cos) + (rotate_half(x) * sin)\n",
        "\n",
        "class RotaryEmbedding(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Rotary position embeddings based on those in\n",
        "    [RoFormer](https://huggingface.co/docs/transformers/model_doc/roformer). Query and keys are transformed by rotation\n",
        "    matrices which depend on their relative positions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim: int):\n",
        "        super().__init__()\n",
        "        # Generate and save the inverse frequency buffer (non trainable)\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n",
        "        inv_freq = inv_freq\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "\n",
        "        self._seq_len_cached = None\n",
        "        self._cos_cached = None\n",
        "        self._sin_cached = None\n",
        "\n",
        "    def _update_cos_sin_tables(self, x, seq_dimension=2):\n",
        "        seq_len = x.shape[seq_dimension]\n",
        "\n",
        "        # Reset the tables if the sequence length has changed,\n",
        "        # or if we're on a new device (possibly due to tracing for instance)\n",
        "        if seq_len != self._seq_len_cached or self._cos_cached.device != x.device:\n",
        "            self._seq_len_cached = seq_len\n",
        "            t = torch.arange(x.shape[seq_dimension], device=x.device).type_as(self.inv_freq)\n",
        "            freqs = torch.outer(t, self.inv_freq)\n",
        "            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n",
        "\n",
        "            self._cos_cached = emb.cos()[None, None, :, :]\n",
        "            self._sin_cached = emb.sin()[None, None, :, :]\n",
        "\n",
        "        return self._cos_cached, self._sin_cached\n",
        "\n",
        "    def forward(self, q: torch.Tensor, k: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        self._cos_cached, self._sin_cached = self._update_cos_sin_tables(k, seq_dimension=-2)\n",
        "\n",
        "        return (\n",
        "            apply_rotary_pos_emb(q, self._cos_cached, self._sin_cached),\n",
        "            apply_rotary_pos_emb(k, self._cos_cached, self._sin_cached)\n",
        "        )\n",
        "\n",
        "class ESMEmbeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.position_embeddings = nn.Embedding(config.block_size, config.n_embd)\n",
        "\n",
        "    def post_model_init(self):\n",
        "        # Merge both the tokenizer vocabs - Battle of Tokenizers\n",
        "        # That is create a new word embedding of pf_gpts vocab size and configs n_embd\n",
        "        # Get pf GPT Tokenizer\n",
        "        pfgpt_tokenizer = AutoTokenizer.from_pretrained(PFGPT_HF_MODEL_PATH, trust_remote_code=True)\n",
        "        pfgpt_tokenizer.pad_token = pfgpt_tokenizer.eos_token\n",
        "        pfgpt_vocab = pfgpt_tokenizer.get_vocab()\n",
        "\n",
        "        # Get ESM Tokenizer\n",
        "        esm_tokenizer = AutoTokenizer.from_pretrained(self.config.pre_trained_model_name, padding='max_length', max_length=1026)\n",
        "        esm_vocab = esm_tokenizer.get_vocab()\n",
        "        new_word_embeddings = nn.Embedding(PFGPT_VOCAB_SIZE, self.config.n_embd)\n",
        "        torch.nn.init.normal_(new_word_embeddings.weight, std = 0.1263)\n",
        "\n",
        "        # Find all the common keys tokens between esm tokenizer and pf_gpt tokenizer\n",
        "        pfgpt_keys = set(pfgpt_vocab.keys())\n",
        "        esm_keys = set(esm_vocab.keys())\n",
        "        common_keys = list(pfgpt_keys.intersection(esm_keys))\n",
        "\n",
        "        # now, copy a particular tokens embedding from ems_embedding to the new embedding that we create here\n",
        "        with torch.no_grad():\n",
        "            indices = []\n",
        "            for key in common_keys:\n",
        "                esm_embd_index = esm_tokenizer.convert_tokens_to_ids(key)\n",
        "                pfg_embd_index = pfgpt_tokenizer.convert_tokens_to_ids(key)\n",
        "                indices.append(pfg_embd_index)\n",
        "                new_word_embeddings.weight[pfg_embd_index] = self.word_embeddings.weight[esm_embd_index]\n",
        "\n",
        "            # Check for embedding equivalance\n",
        "            assert torch.equal(new_word_embeddings.weight[pfgpt_tokenizer.convert_tokens_to_ids(common_keys)],\n",
        "                               self.word_embeddings.weight[esm_tokenizer.convert_tokens_to_ids(common_keys)])\n",
        "\n",
        "        # Create a mask for all the indecis we have copied pretrained embeddings\n",
        "        # and turn requires_grad off to those embeddings that we have copied - This is\n",
        "        # not possible, so instead we store the indecis and zero out the grads before optim.step()\n",
        "        # hence we do not update these embeddings\n",
        "        self.indices = indices\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.word_embeddings = new_word_embeddings\n",
        "        self.word_embeddings.requires_grad_(True)\n",
        "\n",
        "    def forward(self, x, attention_mask = None):\n",
        "        token_embs = self.word_embeddings(x)\n",
        "        # Not required as we are use rotary embeddings - Hence we do not require absolute position embeddings\n",
        "        # position_embs = self.esm.embeddings.position_embeddings(torch.arange(0, x.shape[1], 1, dtype = torch.long))\n",
        "        if attention_mask is not None:\n",
        "            token_embs = (token_embs * attention_mask.unsqueeze(-1)).to(token_embs.dtype)\n",
        "        return token_embs\n",
        "\n",
        "@dataclass\n",
        "class ESMConfig():\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50257\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    hidden_size: int = 4096 # 4 * block_size\n",
        "    dropout: float = 0.0\n",
        "    pre_trained_model_name: str = ''\n",
        "\n",
        "class ESMIntermediateLayer(nn.Module):\n",
        "    def __init__(self, nin, nout, lora_config, dropout = 0.0, ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dense = nn.Linear(nin, nout) if not lora_config.lora_mlp else LoRALinear(nin, nout, lora_config)\n",
        "        self.act = nn.GELU(approximate = 'tanh')\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.dense(x))\n",
        "\n",
        "class ESMOutLayer(nn.Module):\n",
        "    def __init__(self, nin, nout, lora_config, dropout = 0.0, inside_attention = False):\n",
        "        super().__init__()\n",
        "\n",
        "        # 2 places used - 1. inisde the attention block  and inside the MLP\n",
        "        # if used inside attention and lora_config.lora_projection is true then dense is a LoRALInear\n",
        "        # elif used in mlp and lora_config.lora_mlp is true then dens is a LoRALinear again\n",
        "        # else its a Linear\n",
        "\n",
        "        if inside_attention == True and lora_config.lora_projection:\n",
        "            self.dense = LoRALinear(nin, nout, lora_config)\n",
        "        elif inside_attention == False and lora_config.lora_mlp:\n",
        "            self.dense = LoRALinear(nin, nout, lora_config)\n",
        "        else:\n",
        "            self.dense = nn.Linear(nin, nout)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, attn_scores):\n",
        "        x = self.dense(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + attn_scores\n",
        "        return x\n",
        "\n",
        "class ESMSelfAttn(nn.Module): # Verified\n",
        "\n",
        "    def __init__(self, config, lora_config):\n",
        "        super().__init__()\n",
        "\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "\n",
        "        self.query = nn.Linear(config.n_embd, config.n_embd) if not lora_config.lora_query else LoRALinear(config.n_embd, config.n_embd, lora_config)\n",
        "        self.key = nn.Linear(config.n_embd, config.n_embd) if not lora_config.lora_key else LoRALinear(config.n_embd, config.n_embd, lora_config)\n",
        "        self.value = nn.Linear(config.n_embd, config.n_embd) if not lora_config.lora_value else LoRALinear(config.n_embd, config.n_embd, lora_config)\n",
        "        self.n_head = config.n_head\n",
        "\n",
        "        attention_head_size = config.n_embd//config.n_head\n",
        "\n",
        "        # Add a rotary embeddings here\n",
        "        self.rotary_embeddings = RotaryEmbedding(dim = attention_head_size)\n",
        "\n",
        "    def forward(self, x, attention_mask):\n",
        "\n",
        "        # x -> (b, s, e) -> (b s, h, e/h)\n",
        "        k, q, v = self.key(x), self.query(x), self.value(x)\n",
        "\n",
        "        k = rearrange(k, 'b s (h e) -> b h s e', h = self.n_head)\n",
        "        q = rearrange(q, 'b s (h e) -> b h s e', h = self.n_head)\n",
        "        v = rearrange(v, 'b s (h e) -> b h s e', h = self.n_head)\n",
        "\n",
        "        # Add rotary embeddings here for k and q tensors\n",
        "        q, k = self.rotary_embeddings(q, k)\n",
        "\n",
        "        # Attention claculation - # TODO: make is_casual true in case of finetuning - Very important\n",
        "        y = F.scaled_dot_product_attention(q, k, v, attn_mask = attention_mask, is_causal = False) # flash attention\n",
        "        y = rearrange(y, 'b h s e -> b s (h e)', h = self.n_head)\n",
        "        return y\n",
        "\n",
        "class ESMAttn(nn.Module): # Verified\n",
        "\n",
        "    def __init__(self, config, lora_config):\n",
        "        super().__init__() # No activation function at this level\n",
        "        self.self = ESMSelfAttn(config, lora_config)\n",
        "        self.output = ESMOutLayer(config.n_embd, config.n_embd, lora_config, dropout = getattr(config, 'dropout', 0.), inside_attention = True)\n",
        "        self.LayerNorm = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "    def forward(self, x, attention_mask):\n",
        "        inter_x = self.LayerNorm(x)\n",
        "        attn = self.self(inter_x, attention_mask)\n",
        "        out = self.output(attn, x)\n",
        "        return out\n",
        "\n",
        "class ESMLayers(nn.Module): # Both Init and Forward Verified - Done and Dusted\n",
        "\n",
        "    def __init__(self, config, lora_config):\n",
        "        super().__init__()\n",
        "        self.attention = ESMAttn(config, lora_config)\n",
        "        self.intermediate = ESMIntermediateLayer(config.n_embd, config.hidden_size, lora_config) #\n",
        "        self.output = ESMOutLayer(config.hidden_size, config.n_embd, lora_config) #\n",
        "        self.LayerNorm = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "    def forward(self, x, attention_mask):\n",
        "        attention_op = self.attention(x, attention_mask)\n",
        "        attention_op_ln = self.LayerNorm(attention_op) # This will keep the activations in check - Lets see\n",
        "        inter = self.intermediate(attention_op_ln)\n",
        "        out = self.output(inter, attention_op)\n",
        "        return out\n",
        "\n",
        "class ESMEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, config, lora_config):\n",
        "        super().__init__()\n",
        "\n",
        "        # No activation functions here as well\n",
        "\n",
        "        self.layer = nn.ModuleList([ESMLayers(config, lora_config) for _ in range(config.n_layer)])\n",
        "        self.emb_layer_norm_after = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "    def forward(self, x, attention_mask = None):\n",
        "\n",
        "        for layer in self.layer:\n",
        "            x = layer(x, attention_mask)\n",
        "\n",
        "        return self.emb_layer_norm_after(x)\n",
        "\n",
        "class ESM(nn.Module):\n",
        "\n",
        "    def __init__(self, config, lora_config):\n",
        "\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.esm = nn.ModuleDict(dict(\n",
        "            embeddings = ESMEmbeddings(config),\n",
        "            encoder = ESMEncoder(config, lora_config), # Done, forward - here\n",
        "            final_layer = nn.Linear(config.n_embd, config.vocab_size) if not lora_config.lora_head else\n",
        "                              LoRALinear(config.n_embd, config.vocab_size, lora_config)\n",
        "        ))\n",
        "        self.esm.final_layer.weight = self.esm.embeddings.word_embeddings.weight\n",
        "\n",
        "        # Final Layer bias initializtion\n",
        "        torch.nn.init.zeros_(self.esm.final_layer.bias) # Set the bias to 0\n",
        "        # Finally one small thing is to decide weather to add an intermediate layer or not? - Thats a future discussion\n",
        "\n",
        "    @classmethod\n",
        "    def get_pretrained_config(cls, model_type = 'esm2_t33_650M_UR50D'):\n",
        "\n",
        "        '''\n",
        "        name                n_layers    n_params\n",
        "        esm2_t48_15B_UR50D\t48\t        15B\n",
        "        esm2_t36_3B_UR50D\t36\t        3B\n",
        "        esm2_t33_650M_UR50D\t33\t        650M\n",
        "        esm2_t30_150M_UR50D\t30\t        150M\n",
        "        esm2_t12_35M_UR50D\t12\t        35M\n",
        "        esm2_t6_8M_UR50D\n",
        "        '''\n",
        "\n",
        "        assert model_type in {'esm2_t36_3B_UR50D', 'esm2_t33_650M_UR50D', 'esm2_t30_150M_UR50D'}\n",
        "\n",
        "        config_args = {\n",
        "            'esm2_t36_3B_UR50D': dict(n_layer=36, n_head = 40, n_embd=2560, hidden_size=10240), # 3B params\n",
        "            'esm2_t33_650M_UR50D': dict(n_layer=33, n_head = 20, n_embd=1280, hidden_size=5120), # 650M params\n",
        "            'esm2_t30_150M_UR50D': dict(n_layer=30, n_head = 20, n_embd=640, hidden_size=2560), # 150M params\n",
        "        }[model_type]\n",
        "\n",
        "        config_args['vocab_size'] = 33 # always 33 for ESM Models\n",
        "        config_args['block_size'] = 1026 # Always constant for ESM Models\n",
        "        config_args['pre_trained_model_name'] = f\"facebook/{model_type}\"\n",
        "\n",
        "        config = ESMConfig(**config_args)\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, lora_config, model_type = 'esm2_t33_650M_UR50D', embedding_post_init = True):\n",
        "\n",
        "        config = cls.get_pretrained_config(model_type)\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        model = cls(config, lora_config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        from transformers import AutoModelForSequenceClassification\n",
        "        num_labels = 33\n",
        "        model_hf = AutoModelForSequenceClassification.from_pretrained(config.pre_trained_model_name, num_labels = num_labels)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if 'inv_freq' not in k]\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if 'classifier' not in k]\n",
        "\n",
        "        ignore_keys = ['esm.contact_head.regression.weight', 'esm.contact_head.regression.bias']\n",
        "        for k in sd_keys_hf:\n",
        "\n",
        "            if k in ignore_keys: continue\n",
        "\n",
        "            # vanilla copy over the other parameters\n",
        "            try: assert sd_hf[k].shape == sd[k].shape\n",
        "            except Exception as e:\n",
        "              print(k)\n",
        "              print(f\"Mismatch in the shape of tensor while loading weights - Key: {k}, expected shape: {sd_hf[k].shape}, actual shape: {sd[k].shape if k in sd else k}\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        # Set the final layers bias as 0 so that it does not affect weight tying scheme\n",
        "        with torch.no_grad():\n",
        "            model.esm.final_layer.bias.zero_()\n",
        "\n",
        "        # Freeze the model\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'lora_' not in name:\n",
        "                param.requires_grad = False\n",
        "\n",
        "        if embedding_post_init:\n",
        "            model.esm.embeddings.post_model_init()\n",
        "            del model.esm.final_layer\n",
        "\n",
        "            #IMP: Here we are assuming that embeddings will never have LoRA attached to it, hence we are going with Linear\n",
        "            model.esm.final_layer = nn.Linear(config.n_embd, model.esm.embeddings.word_embeddings.weight.shape[0])\n",
        "\n",
        "            model.esm.final_layer.weight = model.esm.embeddings.word_embeddings.weight\n",
        "\n",
        "        return model\n",
        "\n",
        "    def get_extended_attn_mask(self, attention_mask, input_shape):\n",
        "\n",
        "        if attention_mask == None: return None\n",
        "        b, s = attention_mask.shape\n",
        "        # Make the attention mask braodcastable for [batch_size, n_heads, seq_len, seq_len]\n",
        "        attention_mask = attention_mask[:, None, None, :]\n",
        "\n",
        "        # Now make sure that it has negetive infinity for all the padded tokens and\n",
        "        # 0 for all attention tokens as we add this mask to attention scores\n",
        "        attn_mask = attention_mask.to(torch.float32)\n",
        "        attn_mask = (1 - attn_mask) * (torch.finfo(torch.float32).min)\n",
        "        attn_mask = attn_mask.expand(b, 1, s, s)\n",
        "        return attn_mask\n",
        "\n",
        "    def forward(self, x, y = None, attention_mask = None, output_encoder_states = True):\n",
        "\n",
        "        # Calculate Embeddings\n",
        "        x = self.esm.embeddings(x, attention_mask) # TODO: Verify the new embeddings function without doing post init and after doing post model init - Ideally both should stay the same\n",
        "\n",
        "        # compute attention_mask for attention scores\n",
        "        extended_attention_mask = self.get_extended_attn_mask(attention_mask, x.shape)\n",
        "\n",
        "        #Do the forward pass\n",
        "        x = self.esm.encoder(x, attention_mask = extended_attention_mask)\n",
        "        logits = self.esm.final_layer(x)\n",
        "        output = {'logits': logits}\n",
        "\n",
        "        if output_encoder_states:\n",
        "            output['encoder_output'] = x\n",
        "        if y is not None:\n",
        "            # Calculate loss and send it in output\n",
        "            outputs = logits.view(-1, logits.size(-1))  # (bs*seq_len, 384)\n",
        "            targets = y.view(-1)  # (bs*seq_len)\n",
        "\n",
        "            # Flatten the attention mask\n",
        "            attention_mask = attention_mask.view(-1)  # (bs*seq_len)\n",
        "\n",
        "            # Calculate cross entropy loss\n",
        "            loss = F.cross_entropy(outputs, targets, reduction='none')\n",
        "\n",
        "            # Apply the mask to the loss\n",
        "            masked_loss = loss * attention_mask\n",
        "\n",
        "            # Calculate the mean loss over the actual tokens (excluding padding)\n",
        "            total_loss = masked_loss.sum()\n",
        "            num_tokens = attention_mask.sum()\n",
        "\n",
        "            actual_loss = total_loss / num_tokens\n",
        "            output['loss'] = actual_loss\n",
        "\n",
        "        return output\n",
        "\n",
        "ds = load_dataset(\"lamm-mit/GPTProteinPretrained\")\n",
        "tokenizer = get_tokenizer()\n",
        "\n",
        "class ProtDS(Dataset):\n",
        "    def __init__(self, sequences, max_len = 1026):\n",
        "\n",
        "        self.sequences = sequences # list object\n",
        "        self.max_len = max_len\n",
        "        self.eos_token = '</s>' # PFGPT's eos token - End of sequence\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.sequences[idx]\n",
        "        if len(seq) >= self.max_len - 1:\n",
        "            seq = seq[:self.max_len - 1]\n",
        "\n",
        "        label = seq[1:] + self.eos_token\n",
        "\n",
        "        return seq, label\n",
        "\n",
        "def seq_collate_fn(data):\n",
        "    # data is a list of tuples\n",
        "    x, y = zip(*data)\n",
        "    train_tokenized = tokenizer(x)\n",
        "    labels_tokenized = tokenizer(y)\n",
        "\n",
        "    padded_train_tokenized = tokenizer.pad(train_tokenized, padding = 'max_length', max_length = 1026)\n",
        "    padded_labels_tokenized = tokenizer.pad(labels_tokenized, padding = 'max_length', max_length = 1026)\n",
        "    padded_train_tokenized['labels'] = padded_labels_tokenized['input_ids']\n",
        "\n",
        "    for k, v in padded_train_tokenized.items():\n",
        "        try:\n",
        "            padded_train_tokenized[k] = torch.tensor(v, dtype = torch.long)\n",
        "        except:\n",
        "            breakpoint()\n",
        "\n",
        "    return padded_train_tokenized\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-lVaIU243_b",
        "outputId": "aa48744d-7181-4536-ecd2-305ba81feb6f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "Uc-YQbIr6uO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ds['train']['text']\n",
        "train_seqs, valid_seqs = train_test_split(sequences, test_size = 0.05, shuffle = True)\n",
        "train_ds, valid_ds = ProtDS(train_seqs), ProtDS(valid_seqs)"
      ],
      "metadata": {
        "id": "pVRG1oux6wnP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PFGPT Benchmarking"
      ],
      "metadata": {
        "id": "bXe7o0YS6jZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "AcmQ_cw0AED5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "ForceGPT_model_name='lamm-mit/ProteinForceGPT'\n",
        "\n",
        "pf_tokenizer = AutoTokenizer.from_pretrained(ForceGPT_model_name, trust_remote_code=True)\n",
        "pf_tokenizer.pad_token = pf_tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    ForceGPT_model_name,\n",
        "    trust_remote_code=True\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "5kk3U9Tg-1ap"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pf_seq_collate_fn(data):\n",
        "    # data is a list of tuples\n",
        "    x, y = zip(*data)\n",
        "    train_tokenized = pf_tokenizer(x)\n",
        "    labels_tokenized = pf_tokenizer(y)\n",
        "\n",
        "    padded_train_tokenized = pf_tokenizer.pad(train_tokenized, padding = 'max_length', max_length = 1026)\n",
        "    padded_labels_tokenized = pf_tokenizer.pad(labels_tokenized, padding = 'max_length', max_length = 1026)\n",
        "    padded_train_tokenized['labels'] = padded_labels_tokenized['input_ids']\n",
        "\n",
        "    for k, v in padded_train_tokenized.items():\n",
        "        try:\n",
        "            padded_train_tokenized[k] = torch.tensor(v, dtype = torch.long)[:, :1024]\n",
        "        except:\n",
        "            breakpoint()\n",
        "\n",
        "    return padded_train_tokenized\n",
        "valid_dl = DataLoader(valid_ds, batch_size = 4, collate_fn = pf_seq_collate_fn)\n",
        "batch = next(iter(valid_dl))"
      ],
      "metadata": {
        "id": "zHDEOoqoCaGW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch['input_ids'].shape, batch['attention_mask'].shape, batch['labels'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZvEYckmD_Ga",
        "outputId": "03138e3f-0b83-4e0c-f120-b343e321275f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 1024]), torch.Size([4, 1024]), torch.Size([4, 1024]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSZU0CFyEeF3",
        "outputId": "3553345f-7983-4d84-ab25-7fe7fe04b665"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTNeoXForCausalLM(\n",
              "  (gpt_neox): GPTNeoXModel(\n",
              "    (embed_in): Embedding(384, 1024)\n",
              "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-35): 36 x GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXSdpaAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (embed_out): Linear(in_features=1024, out_features=384, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dl = DataLoader(valid_ds, batch_size = 64, collate_fn = pf_seq_collate_fn)\n",
        "\n",
        "with torch.no_grad():\n",
        "    vl_losses = []\n",
        "    for it, batch in enumerate(valid_dl):\n",
        "        # breakpoint()\n",
        "        out = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
        "        outputs = out.logits.view(-1, out.logits.size(-1))  # (bs*seq_len, 384)\n",
        "        targets = batch['labels'].reshape(-1)  # (bs*seq_len)\n",
        "\n",
        "        # Flatten the attention mask\n",
        "        attention_mask = batch['attention_mask'].reshape(-1)  # (bs*seq_len)\n",
        "\n",
        "        # Calculate cross entropy loss\n",
        "        loss = F.cross_entropy(outputs, targets.to(device))#, reduction='none')\n",
        "\n",
        "        # Apply the mask to the loss\n",
        "        # masked_loss = loss * attention_mask.to(device)\n",
        "\n",
        "        # # Calculate the mean loss over the actual tokens (excluding padding)\n",
        "        # total_loss = masked_loss.sum()\n",
        "        # num_tokens = attention_mask.sum()\n",
        "\n",
        "        # actual_loss = total_loss / num_tokens\n",
        "        # vl_losses.append(actual_loss)\n",
        "        vl_losses.append(loss.item())\n",
        "        progress = it/len(valid_dl)\n",
        "        print(f\"\\r Valid Progress: {progress:.5f}%, train loss: {vl_losses[-1]:.6f}\", end='')\n",
        "\n",
        "        if (it+1)%200 == 0: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "5DSqR3OcBAuQ",
        "outputId": "704232ef-51e9-4baf-c7f1-5a90341275bd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Valid Progress: 0.22020%, train loss: 2.241032"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-cb1847c7ea2b>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Calculate cross entropy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, reduction='none')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Apply the mask to the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(vl_losses)"
      ],
      "metadata": {
        "id": "NCB5W0vZUjaw",
        "outputId": "a86e093d-12c2-4aeb-c1b1-7aa74bbf24f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.042728337778974"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_sequences = valid_ds.sequences[:64]"
      ],
      "metadata": {
        "id": "1N1vRZOcUrR2"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_seqs = [pf_tokenizer.encode(target_sequences[0], add_special_tokens = False) for i in target_sequences]"
      ],
      "metadata": {
        "id": "xxU7dZ1yVCOo",
        "outputId": "59a6f120-8b53-470e-8dac-046736e77b2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[86,\n",
              " 104,\n",
              " 116,\n",
              " 120,\n",
              " 104,\n",
              " 113,\n",
              " 102,\n",
              " 104,\n",
              " 63,\n",
              " 80,\n",
              " 72,\n",
              " 84,\n",
              " 71,\n",
              " 78,\n",
              " 72,\n",
              " 81,\n",
              " 78,\n",
              " 84,\n",
              " 86,\n",
              " 79,\n",
              " 83,\n",
              " 86,\n",
              " 78,\n",
              " 92,\n",
              " 81,\n",
              " 83,\n",
              " 78,\n",
              " 72,\n",
              " 89,\n",
              " 72,\n",
              " 72,\n",
              " 74,\n",
              " 85,\n",
              " 92,\n",
              " 84,\n",
              " 73,\n",
              " 90,\n",
              " 79,\n",
              " 71,\n",
              " 74,\n",
              " 78,\n",
              " 73,\n",
              " 73,\n",
              " 72,\n",
              " 68,\n",
              " 87,\n",
              " 74,\n",
              " 71,\n",
              " 83,\n",
              " 71,\n",
              " 78,\n",
              " 72,\n",
              " 83,\n",
              " 92,\n",
              " 86,\n",
              " 76,\n",
              " 89,\n",
              " 76,\n",
              " 83,\n",
              " 83,\n",
              " 83,\n",
              " 81,\n",
              " 89,\n",
              " 87,\n",
              " 74,\n",
              " 85,\n",
              " 79,\n",
              " 75,\n",
              " 79,\n",
              " 74,\n",
              " 75,\n",
              " 68,\n",
              " 90,\n",
              " 71,\n",
              " 87,\n",
              " 86,\n",
              " 80,\n",
              " 84,\n",
              " 71,\n",
              " 87,\n",
              " 76,\n",
              " 87,\n",
              " 85,\n",
              " 80,\n",
              " 78,\n",
              " 85,\n",
              " 80,\n",
              " 84,\n",
              " 74,\n",
              " 92,\n",
              " 71,\n",
              " 89,\n",
              " 79,\n",
              " 90,\n",
              " 79,\n",
              " 83,\n",
              " 74,\n",
              " 80,\n",
              " 71,\n",
              " 75,\n",
              " 68,\n",
              " 74,\n",
              " 76,\n",
              " 68,\n",
              " 87,\n",
              " 84,\n",
              " 68,\n",
              " 78,\n",
              " 89,\n",
              " 72,\n",
              " 68,\n",
              " 85,\n",
              " 79,\n",
              " 78,\n",
              " 72,\n",
              " 86,\n",
              " 74,\n",
              " 87,\n",
              " 81,\n",
              " 85,\n",
              " 92,\n",
              " 72,\n",
              " 79,\n",
              " 74,\n",
              " 85,\n",
              " 72,\n",
              " 78,\n",
              " 73,\n",
              " 79,\n",
              " 72,\n",
              " 78,\n",
              " 68,\n",
              " 90,\n",
              " 72,\n",
              " 90,\n",
              " 78,\n",
              " 72,\n",
              " 72,\n",
              " 92,\n",
              " 68,\n",
              " 68,\n",
              " 73,\n",
              " 76,\n",
              " 85,\n",
              " 86,\n",
              " 84,\n",
              " 90,\n",
              " 72,\n",
              " 78,\n",
              " 79,\n",
              " 74,\n",
              " 79,\n",
              " 74,\n",
              " 79,\n",
              " 71,\n",
              " 92,\n",
              " 86,\n",
              " 85,\n",
              " 72,\n",
              " 85,\n",
              " 73,\n",
              " 87,\n",
              " 79,\n",
              " 71,\n",
              " 72,\n",
              " 74,\n",
              " 79,\n",
              " 86,\n",
              " 71,\n",
              " 68,\n",
              " 89,\n",
              " 85,\n",
              " 72,\n",
              " 89,\n",
              " 73,\n",
              " 89,\n",
              " 78,\n",
              " 79,\n",
              " 92,\n",
              " 72,\n",
              " 78,\n",
              " 74,\n",
              " 79,\n",
              " 76,\n",
              " 92,\n",
              " 85,\n",
              " 74,\n",
              " 72,\n",
              " 92,\n",
              " 76,\n",
              " 76,\n",
              " 81,\n",
              " 90,\n",
              " 71,\n",
              " 83,\n",
              " 86,\n",
              " 87,\n",
              " 78,\n",
              " 87,\n",
              " 68,\n",
              " 79,\n",
              " 86,\n",
              " 71,\n",
              " 76,\n",
              " 72,\n",
              " 89,\n",
              " 76,\n",
              " 92,\n",
              " 72,\n",
              " 72,\n",
              " 76,\n",
              " 84,\n",
              " 74,\n",
              " 78,\n",
              " 73,\n",
              " 92,\n",
              " 75,\n",
              " 80,\n",
              " 85,\n",
              " 92,\n",
              " 83,\n",
              " 76,\n",
              " 78,\n",
              " 74,\n",
              " 86,\n",
              " 71,\n",
              " 72,\n",
              " 87,\n",
              " 76,\n",
              " 72,\n",
              " 76,\n",
              " 68,\n",
              " 87,\n",
              " 87,\n",
              " 85,\n",
              " 83,\n",
              " 72,\n",
              " 87,\n",
              " 80,\n",
              " 79,\n",
              " 74,\n",
              " 71,\n",
              " 87,\n",
              " 68,\n",
              " 89,\n",
              " 68,\n",
              " 89,\n",
              " 75,\n",
              " 83,\n",
              " 78,\n",
              " 71,\n",
              " 72,\n",
              " 85,\n",
              " 92,\n",
              " 84,\n",
              " 75,\n",
              " 79,\n",
              " 76,\n",
              " 74,\n",
              " 78,\n",
              " 87,\n",
              " 89,\n",
              " 76,\n",
              " 79,\n",
              " 83,\n",
              " 76,\n",
              " 89,\n",
              " 74,\n",
              " 85,\n",
              " 72,\n",
              " 76,\n",
              " 72,\n",
              " 76,\n",
              " 89,\n",
              " 68,\n",
              " 71,\n",
              " 71,\n",
              " 92,\n",
              " 89,\n",
              " 71,\n",
              " 80,\n",
              " 72,\n",
              " 79,\n",
              " 74,\n",
              " 86,\n",
              " 74,\n",
              " 68,\n",
              " 89,\n",
              " 78,\n",
              " 76,\n",
              " 87,\n",
              " 83,\n",
              " 68,\n",
              " 75,\n",
              " 71,\n",
              " 83,\n",
              " 81,\n",
              " 71,\n",
              " 73,\n",
              " 72,\n",
              " 76,\n",
              " 74,\n",
              " 81,\n",
              " 85,\n",
              " 75,\n",
              " 84,\n",
              " 79,\n",
              " 72,\n",
              " 85,\n",
              " 76,\n",
              " 79,\n",
              " 89,\n",
              " 80,\n",
              " 81,\n",
              " 72,\n",
              " 71,\n",
              " 74,\n",
              " 86,\n",
              " 80,\n",
              " 81,\n",
              " 72,\n",
              " 81,\n",
              " 68,\n",
              " 74,\n",
              " 78,\n",
              " 92,\n",
              " 84,\n",
              " 74,\n",
              " 79,\n",
              " 71,\n",
              " 85,\n",
              " 73,\n",
              " 72,\n",
              " 70,\n",
              " 85,\n",
              " 78,\n",
              " 84,\n",
              " 76,\n",
              " 89,\n",
              " 78,\n",
              " 71,\n",
              " 79,\n",
              " 78,\n",
              " 72,\n",
              " 84,\n",
              " 74,\n",
              " 89,\n",
              " 80,\n",
              " 73,\n",
              " 81,\n",
              " 76,\n",
              " 72,\n",
              " 72,\n",
              " 85,\n",
              " 87,\n",
              " 75,\n",
              " 84,\n",
              " 89,\n",
              " 74,\n",
              " 75,\n",
              " 86,\n",
              " 72,\n",
              " 85,\n",
              " 86,\n",
              " 74,\n",
              " 68,\n",
              " 89,\n",
              " 89,\n",
              " 72,\n",
              " 83,\n",
              " 92,\n",
              " 79,\n",
              " 86,\n",
              " 87,\n",
              " 84,\n",
              " 90,\n",
              " 73,\n",
              " 89,\n",
              " 78,\n",
              " 80,\n",
              " 71,\n",
              " 83,\n",
              " 79,\n",
              " 68,\n",
              " 78,\n",
              " 86,\n",
              " 68,\n",
              " 79,\n",
              " 71,\n",
              " 80,\n",
              " 84,\n",
              " 68,\n",
              " 71,\n",
              " 68,\n",
              " 71,\n",
              " 72,\n",
              " 78,\n",
              " 89,\n",
              " 81,\n",
              " 73,\n",
              " 89,\n",
              " 83,\n",
              " 71,\n",
              " 85,\n",
              " 73,\n",
              " 72,\n",
              " 85,\n",
              " 87,\n",
              " 92,\n",
              " 73,\n",
              " 81,\n",
              " 90,\n",
              " 80,\n",
              " 71,\n",
              " 81,\n",
              " 76,\n",
              " 85,\n",
              " 71,\n",
              " 90,\n",
              " 70,\n",
              " 76,\n",
              " 86,\n",
              " 85,\n",
              " 84,\n",
              " 79,\n",
              " 90,\n",
              " 90,\n",
              " 74,\n",
              " 75,\n",
              " 85,\n",
              " 76,\n",
              " 83,\n",
              " 68,\n",
              " 90,\n",
              " 92,\n",
              " 75,\n",
              " 78,\n",
              " 72,\n",
              " 87,\n",
              " 78,\n",
              " 72,\n",
              " 76,\n",
              " 92,\n",
              " 89,\n",
              " 74,\n",
              " 78,\n",
              " 72,\n",
              " 68,\n",
              " 83,\n",
              " 68,\n",
              " 71,\n",
              " 76,\n",
              " 72,\n",
              " 81,\n",
              " 90,\n",
              " 72,\n",
              " 84,\n",
              " 71,\n",
              " 72,\n",
              " 71,\n",
              " 89,\n",
              " 79,\n",
              " 71,\n",
              " 87,\n",
              " 90,\n",
              " 73,\n",
              " 86,\n",
              " 86,\n",
              " 68,\n",
              " 79,\n",
              " 90,\n",
              " 83,\n",
              " 73,\n",
              " 86,\n",
              " 87,\n",
              " 80,\n",
              " 74,\n",
              " 90,\n",
              " 83,\n",
              " 81,\n",
              " 72,\n",
              " 71,\n",
              " 86,\n",
              " 68,\n",
              " 71,\n",
              " 79,\n",
              " 78,\n",
              " 85,\n",
              " 92,\n",
              " 73,\n",
              " 83,\n",
              " 87,\n",
              " 81,\n",
              " 89,\n",
              " 79,\n",
              " 89,\n",
              " 87,\n",
              " 74,\n",
              " 92,\n",
              " 71,\n",
              " 76,\n",
              " 76,\n",
              " 73,\n",
              " 73,\n",
              " 90,\n",
              " 89,\n",
              " 86,\n",
              " 85,\n",
              " 80,\n",
              " 76,\n",
              " 73,\n",
              " 84,\n",
              " 86,\n",
              " 78,\n",
              " 72,\n",
              " 73,\n",
              " 80,\n",
              " 81,\n",
              " 72,\n",
              " 78,\n",
              " 83,\n",
              " 73,\n",
              " 72,\n",
              " 71,\n",
              " 87,\n",
              " 79,\n",
              " 79,\n",
              " 75,\n",
              " 74,\n",
              " 79,\n",
              " 76,\n",
              " 85,\n",
              " 71,\n",
              " 68,\n",
              " 71,\n",
              " 74,\n",
              " 85,\n",
              " 78,\n",
              " 80,\n",
              " 86,\n",
              " 78,\n",
              " 86,\n",
              " 79,\n",
              " 74,\n",
              " 81,\n",
              " 74,\n",
              " 89,\n",
              " 71,\n",
              " 83,\n",
              " 80,\n",
              " 71,\n",
              " 89,\n",
              " 76,\n",
              " 72,\n",
              " 78,\n",
              " 92,\n",
              " 74,\n",
              " 68,\n",
              " 71,\n",
              " 86,\n",
              " 79,\n",
              " 85,\n",
              " 92,\n",
              " 73,\n",
              " 79,\n",
              " 80,\n",
              " 87,\n",
              " 74,\n",
              " 86,\n",
              " 87,\n",
              " 83,\n",
              " 74,\n",
              " 84,\n",
              " 71,\n",
              " 79,\n",
              " 85,\n",
              " 73,\n",
              " 75,\n",
              " 90,\n",
              " 72,\n",
              " 78,\n",
              " 89,\n",
              " 72,\n",
              " 86,\n",
              " 87,\n",
              " 90,\n",
              " 81,\n",
              " 73,\n",
              " 68,\n",
              " 81,\n",
              " 78,\n",
              " 89,\n",
              " 90,\n",
              " 81,\n",
              " 68,\n",
              " 86,\n",
              " 85,\n",
              " 73,\n",
              " 86,\n",
              " 76,\n",
              " 80,\n",
              " 81,\n",
              " 80,\n",
              " 72,\n",
              " 74,\n",
              " 73,\n",
              " 87,\n",
              " 92,\n",
              " 72,\n",
              " 71,\n",
              " 76,\n",
              " 71,\n",
              " 79,\n",
              " 87,\n",
              " 74,\n",
              " 72,\n",
              " 79,\n",
              " 86,\n",
              " 79,\n",
              " 83,\n",
              " 71,\n",
              " 85,\n",
              " 90,\n",
              " 76,\n",
              " 79,\n",
              " 68,\n",
              " 85,\n",
              " 79,\n",
              " 81,\n",
              " 72,\n",
              " 87,\n",
              " 76,\n",
              " 72,\n",
              " 84,\n",
              " 89,\n",
              " 87,\n",
              " 85,\n",
              " 81,\n",
              " 86,\n",
              " 81,\n",
              " 78,\n",
              " 92,\n",
              " 72,\n",
              " 73,\n",
              " 74,\n",
              " 72,\n",
              " 68,\n",
              " 74,\n",
              " 85,\n",
              " 75,\n",
              " 79,\n",
              " 92,\n",
              " 81,\n",
              " 73,\n",
              " 76,\n",
              " 90,\n",
              " 71,\n",
              " 72,\n",
              " 73,\n",
              " 70,\n",
              " 71,\n",
              " 90,\n",
              " 92,\n",
              " 76,\n",
              " 72,\n",
              " 80,\n",
              " 68,\n",
              " 78,\n",
              " 79,\n",
              " 86,\n",
              " 79,\n",
              " 92,\n",
              " 74,\n",
              " 72,\n",
              " 71,\n",
              " 72,\n",
              " 81,\n",
              " 78,\n",
              " 78,\n",
              " 78,\n",
              " 87,\n",
              " 87,\n",
              " 85,\n",
              " 86,\n",
              " 89,\n",
              " 79,\n",
              " 68,\n",
              " 75,\n",
              " 89,\n",
              " 79,\n",
              " 71,\n",
              " 84,\n",
              " 87,\n",
              " 80,\n",
              " 85,\n",
              " 80,\n",
              " 79,\n",
              " 75,\n",
              " 83,\n",
              " 73,\n",
              " 80,\n",
              " 83,\n",
              " 73,\n",
              " 76,\n",
              " 87,\n",
              " 72,\n",
              " 72,\n",
              " 76,\n",
              " 90,\n",
              " 84,\n",
              " 84,\n",
              " 79,\n",
              " 83,\n",
              " 75,\n",
              " 72,\n",
              " 74,\n",
              " 83,\n",
              " 86,\n",
              " 76,\n",
              " 87,\n",
              " 89,\n",
              " 86,\n",
              " 78,\n",
              " 90,\n",
              " 83,\n",
              " 72,\n",
              " 89,\n",
              " 81,\n",
              " 86,\n",
              " 72,\n",
              " 73,\n",
              " 71,\n",
              " 81,\n",
              " 83,\n",
              " 84,\n",
              " 68,\n",
              " 89,\n",
              " 84,\n",
              " 72,\n",
              " 80,\n",
              " 84,\n",
              " 85,\n",
              " 79,\n",
              " 89,\n",
              " 86,\n",
              " 76,\n",
              " 76,\n",
              " 85,\n",
              " 86,\n",
              " 89,\n",
              " 85,\n",
              " 81,\n",
              " 86,\n",
              " 85,\n",
              " 68,\n",
              " 72,\n",
              " 89,\n",
              " 71,\n",
              " 87,\n",
              " 83,\n",
              " 80,\n",
              " 86,\n",
              " 78,\n",
              " 84,\n",
              " 76,\n",
              " 78,\n",
              " 80,\n",
              " 79,\n",
              " 76,\n",
              " 78,\n",
              " 87,\n",
              " 72,\n",
              " 81,\n",
              " 72,\n",
              " 84,\n",
              " 79,\n",
              " 87,\n",
              " 68,\n",
              " 72,\n",
              " 79,\n",
              " 72,\n",
              " 78,\n",
              " 81,\n",
              " 85,\n",
              " 71,\n",
              " 92,\n",
              " 79,\n",
              " 72,\n",
              " 85,\n",
              " 73,\n",
              " 70,\n",
              " 81,\n",
              " 83,\n",
              " 86,\n",
              " 72,\n",
              " 79,\n",
              " 86,\n",
              " 76,\n",
              " 86,\n",
              " 87,\n",
              " 89,\n",
              " 76,\n",
              " 72,\n",
              " 68,\n",
              " 83,\n",
              " 71,\n",
              " 78,\n",
              " 68,\n",
              " 80,\n",
              " 87,\n",
              " 86,\n",
              " 89,\n",
              " 89,\n",
              " 87,\n",
              " 74,\n",
              " 68,\n",
              " 72,\n",
              " 76,\n",
              " 73,\n",
              " 79,\n",
              " 83,\n",
              " 79,\n",
              " 72,\n",
              " 74,\n",
              " 79,\n",
              " 76,\n",
              " 71,\n",
              " 73,\n",
              " 71,\n",
              " 78,\n",
              " 72,\n",
              " 76,\n",
              " 78,\n",
              " 85,\n",
              " 79,\n",
              " 72,\n",
              " 81,\n",
              " 72,\n",
              " 79,\n",
              " 68,\n",
              " 78,\n",
              " 90,\n",
              " 87,\n",
              " 78,\n",
              " 72,\n",
              " 89,\n",
              " 72,\n",
              " 85,\n",
              " 89,\n",
              " 84,\n",
              " 78,\n",
              " 78,\n",
              " 79,\n",
              " 86,\n",
              " 81,\n",
              " 84,\n",
              " 74,\n",
              " 73,\n",
              " 89,\n",
              " 86,\n",
              " 78,\n",
              " 68,\n",
              " 83,\n",
              " 72,\n",
              " 86,\n",
              " 89,\n",
              " 89,\n",
              " 72,\n",
              " 72,\n",
              " 72,\n",
              " 78,\n",
              " 85,\n",
              " 78,\n",
              " 72,\n",
              " 78,\n",
              " 71,\n",
              " 92,\n",
              " 79,\n",
              " 71,\n",
              " 78,\n",
              " 84,\n",
              " 68,\n",
              " 78,\n",
              " 89,\n",
              " 78,\n",
              " 87,\n",
              " 85,\n",
              " 79,\n",
              " 86,\n",
              " 72,\n",
              " 79,\n",
              " 78,\n",
              " 65,\n",
              " 1]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated = torch.tensor() .unsqueeze(0).to(device)"
      ],
      "metadata": {
        "id": "aFBIBY79U71f",
        "outputId": "a01644f8-805f-4dcd-a7d5-581608812173",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "type of None unknown: <class 'NoneType'>. Should be one of a python, numpy, pytorch or tensorflow object.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-b88382f0a52b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2823\u001b[0m                 method).\n\u001b[1;32m   2824\u001b[0m         \"\"\"\n\u001b[0;32m-> 2825\u001b[0;31m         encoded_inputs = self.encode_plus(\n\u001b[0m\u001b[1;32m   2826\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3235\u001b[0m         )\n\u001b[1;32m   3236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3237\u001b[0;31m         return self._encode_plus(\n\u001b[0m\u001b[1;32m   3238\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3239\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m         return self.prepare_for_model(\n\u001b[0m\u001b[1;32m    801\u001b[0m             \u001b[0mfirst_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0mpair_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msecond_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mprepare_for_model\u001b[0;34m(self, ids, pair_ids, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[1;32m   3719\u001b[0m         \u001b[0;31m# Padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3720\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpadding_strategy\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDO_NOT_PAD\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreturn_attention_mask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3721\u001b[0;31m             encoded_inputs = self.pad(\n\u001b[0m\u001b[1;32m   3722\u001b[0m                 \u001b[0mencoded_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3723\u001b[0m                 \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3509\u001b[0m                 \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"np\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3510\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3511\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   3512\u001b[0m                     \u001b[0;34mf\"type of {first_element} unknown: {type(first_element)}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3513\u001b[0m                     \u001b[0;34m\"Should be one of a python, numpy, pytorch or tensorflow object.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: type of None unknown: <class 'NoneType'>. Should be one of a python, numpy, pytorch or tensorflow object."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yZNs-1lOU76K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZSyshvI1U7_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7fuLAF_EU8Ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8KVRgzDbU8Pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uod2X58jU8aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "RuZmhlEm5QeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ESM.from_pretrained(LoRAConfig(lora_r = 32, lora_key = True, lora_mlp = True, lora_projection = True, lora_alpha = 16), 'esm2_t30_150M_UR50D')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiRJB-6D5qZf",
        "outputId": "83e46ae0-1b83-4790-9029-b2525876c9fd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading weights from pretrained gpt: esm2_t30_150M_UR50D\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t30_150M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_for_grad_update = 30000\n",
        "epochs = 1\n",
        "batch_size = 24\n",
        "grad_accum_steps = 5\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size = batch_size, collate_fn = seq_collate_fn)\n",
        "valid_dl = DataLoader(valid_ds, batch_size = batch_size, collate_fn = seq_collate_fn)\n",
        "iterations = epochs * len(train_dl) + 5\n",
        "\n",
        "opt = AdamW(model.parameters(), lr = 3e-04, betas = (0.9, 0.95), eps = 1e-05)\n",
        "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr = 6e-04, total_steps = iterations, final_div_factor=10.0)\n",
        "\n",
        "steps_processed_after_gradstep = 0\n",
        "total_tokens_trained = 0\n",
        "loss = None\n",
        "losses = []\n",
        "\n",
        "vl_losses_track = {0: -np.log(1/384)} # Ideal loss at epoch 0 before any finetuning\n",
        "vl_losses_all = []\n",
        "\n",
        "for i in range(epochs):\n",
        "    c = 0\n",
        "    for iter, batch in enumerate(train_dl):\n",
        "\n",
        "        outputs = model(batch['input_ids'].to(device), y = batch['labels'].to(device), attention_mask = batch['attention_mask'].to(device))\n",
        "        total_tokens_trained += batch['attention_mask'].sum()\n",
        "\n",
        "        losses.append(outputs['loss'].item())\n",
        "        outputs['loss'] = outputs['loss']/grad_accum_steps\n",
        "        outputs['loss'].backward()\n",
        "        steps_processed_after_gradstep += 1\n",
        "\n",
        "        if steps_processed_after_gradstep == grad_accum_steps:\n",
        "            # Do a backward pass and optimizer step\n",
        "            norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "            lr_scheduler.step()\n",
        "\n",
        "            steps_processed_after_gradstep = 0\n",
        "            progress = iter/len(train_dl)\n",
        "            print(f\"Train Progress: {progress:.6f}%, train loss: {losses[-1]:.6f}, norm: {norm:.4f}\")\n",
        "            c += 1\n",
        "\n",
        "        if c >= 5: # approximately for every 400k tokens trained on, lets calculate the validation loss\n",
        "            c = 0\n",
        "            vl_losses = []\n",
        "            v_iter = 0\n",
        "            with torch.no_grad():\n",
        "                for vl_batch in valid_dl:\n",
        "                    vl_outputs = model(vl_batch['input_ids'].to(device), y = vl_batch['labels'].to(device), attention_mask = vl_batch['attention_mask'].to(device))\n",
        "                    vl_losses.append(vl_outputs['loss'].item())\n",
        "                    v_iter += 1\n",
        "\n",
        "                    if v_iter%10 == 0: break\n",
        "\n",
        "            vl_losses_all += vl_losses\n",
        "            vl_losses_track[total_tokens_trained] = np.mean(vl_losses)\n",
        "            print(f\"valid loss: {losses[-1]:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XDi1LC8E5S1n",
        "outputId": "275ae85c-424c-4790-9f53-923f8f7582ef"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Progress: 0.000131%, train loss: 6.474083, norm: 15.2240\n",
            "Train Progress: 0.000295%, train loss: 6.337980, norm: 14.6919\n",
            "Train Progress: 0.000458%, train loss: 6.048813, norm: 18.4991\n",
            "Train Progress: 0.000622%, train loss: 5.744795, norm: 20.5863\n",
            "Train Progress: 0.000785%, train loss: 5.480638, norm: 25.1512\n",
            "valid loss: 5.480638\n",
            "Train Progress: 0.000949%, train loss: 5.136886, norm: 31.5467\n",
            "Train Progress: 0.001113%, train loss: 4.900673, norm: 31.3607\n",
            "Train Progress: 0.001276%, train loss: 4.690476, norm: 32.7555\n",
            "Train Progress: 0.001440%, train loss: 4.494570, norm: 33.0060\n",
            "Train Progress: 0.001603%, train loss: 4.299559, norm: 26.9208\n",
            "valid loss: 4.299559\n",
            "Train Progress: 0.001767%, train loss: 4.093726, norm: 31.4063\n",
            "Train Progress: 0.001931%, train loss: 3.903715, norm: 26.8552\n",
            "Train Progress: 0.002094%, train loss: 3.689974, norm: 38.7530\n",
            "Train Progress: 0.002258%, train loss: 3.488058, norm: 28.6293\n",
            "Train Progress: 0.002421%, train loss: 3.366394, norm: 37.4899\n",
            "valid loss: 3.366394\n",
            "Train Progress: 0.002585%, train loss: 3.161908, norm: 31.6151\n",
            "Train Progress: 0.002749%, train loss: 2.995833, norm: 40.7988\n",
            "Train Progress: 0.002912%, train loss: 2.795163, norm: 31.2358\n",
            "Train Progress: 0.003076%, train loss: 2.593864, norm: 36.7906\n",
            "Train Progress: 0.003240%, train loss: 2.386396, norm: 30.3639\n",
            "valid loss: 2.386396\n",
            "Train Progress: 0.003403%, train loss: 2.253268, norm: 55.8084\n",
            "Train Progress: 0.003567%, train loss: 2.011865, norm: 31.7768\n",
            "Train Progress: 0.003730%, train loss: 1.853546, norm: 36.3985\n",
            "Train Progress: 0.003894%, train loss: 1.726304, norm: 40.4667\n",
            "Train Progress: 0.004058%, train loss: 1.496354, norm: 31.2001\n",
            "valid loss: 1.496354\n",
            "Train Progress: 0.004221%, train loss: 1.364199, norm: 32.5638\n",
            "Train Progress: 0.004385%, train loss: 1.234652, norm: 28.4691\n",
            "Train Progress: 0.004548%, train loss: 1.104195, norm: 40.0209\n",
            "Train Progress: 0.004712%, train loss: 1.008898, norm: 32.1382\n",
            "Train Progress: 0.004876%, train loss: 0.878594, norm: 24.9733\n",
            "valid loss: 0.878594\n",
            "Train Progress: 0.005039%, train loss: 0.760295, norm: 24.0626\n",
            "Train Progress: 0.005203%, train loss: 0.669680, norm: 32.8095\n",
            "Train Progress: 0.005366%, train loss: 0.629235, norm: 27.1542\n",
            "Train Progress: 0.005530%, train loss: 0.517181, norm: 16.9766\n",
            "Train Progress: 0.005694%, train loss: 0.450657, norm: 27.0166\n",
            "valid loss: 0.450657\n",
            "Train Progress: 0.005857%, train loss: 0.396105, norm: 16.0848\n",
            "Train Progress: 0.006021%, train loss: 0.384232, norm: 20.0179\n",
            "Train Progress: 0.006185%, train loss: 0.325224, norm: 11.4425\n",
            "Train Progress: 0.006348%, train loss: 0.258384, norm: 12.6818\n",
            "Train Progress: 0.006512%, train loss: 0.250220, norm: 11.1989\n",
            "valid loss: 0.250220\n",
            "Train Progress: 0.006675%, train loss: 0.211203, norm: 8.8157\n",
            "Train Progress: 0.006839%, train loss: 0.157294, norm: 10.4291\n",
            "Train Progress: 0.007003%, train loss: 0.127757, norm: 8.4321\n",
            "Train Progress: 0.007166%, train loss: 0.112853, norm: 8.8845\n",
            "Train Progress: 0.007330%, train loss: 0.097821, norm: 8.2904\n",
            "valid loss: 0.097821\n",
            "Train Progress: 0.007493%, train loss: 0.074873, norm: 7.4821\n",
            "Train Progress: 0.007657%, train loss: 0.066187, norm: 5.6968\n",
            "Train Progress: 0.007821%, train loss: 0.065802, norm: 4.9993\n",
            "Train Progress: 0.007984%, train loss: 0.041218, norm: 6.0276\n",
            "Train Progress: 0.008148%, train loss: 0.033128, norm: 4.0335\n",
            "valid loss: 0.033128\n",
            "Train Progress: 0.008312%, train loss: 0.027210, norm: 3.7755\n",
            "Train Progress: 0.008475%, train loss: 0.031228, norm: 4.5427\n",
            "Train Progress: 0.008639%, train loss: 0.021179, norm: 2.8945\n",
            "Train Progress: 0.008802%, train loss: 0.020327, norm: 3.8488\n",
            "Train Progress: 0.008966%, train loss: 0.020304, norm: 3.0007\n",
            "valid loss: 0.020304\n",
            "Train Progress: 0.009130%, train loss: 0.013310, norm: 2.7331\n",
            "Train Progress: 0.009293%, train loss: 0.013790, norm: 2.7931\n",
            "Train Progress: 0.009457%, train loss: 0.013118, norm: 2.7895\n",
            "Train Progress: 0.009620%, train loss: 0.010032, norm: 3.2061\n",
            "Train Progress: 0.009784%, train loss: 0.008542, norm: 3.1550\n",
            "valid loss: 0.008542\n",
            "Train Progress: 0.009948%, train loss: 0.009305, norm: 2.4334\n",
            "Train Progress: 0.010111%, train loss: 0.007623, norm: 3.1221\n",
            "Train Progress: 0.010275%, train loss: 0.007090, norm: 4.2036\n",
            "Train Progress: 0.010438%, train loss: 0.006569, norm: 1.9912\n",
            "Train Progress: 0.010602%, train loss: 0.005284, norm: 1.6470\n",
            "valid loss: 0.005284\n",
            "Train Progress: 0.010766%, train loss: 0.005851, norm: 1.6810\n",
            "Train Progress: 0.010929%, train loss: 0.006729, norm: 2.5544\n",
            "Train Progress: 0.011093%, train loss: 0.005730, norm: 2.7119\n",
            "Train Progress: 0.011257%, train loss: 0.005040, norm: 2.0234\n",
            "Train Progress: 0.011420%, train loss: 0.005614, norm: 1.9236\n",
            "valid loss: 0.005614\n",
            "Train Progress: 0.011584%, train loss: 0.003847, norm: 1.7209\n",
            "Train Progress: 0.011747%, train loss: 0.004061, norm: 1.7971\n",
            "Train Progress: 0.011911%, train loss: 0.003631, norm: 1.6310\n",
            "Train Progress: 0.012075%, train loss: 0.004017, norm: 1.4481\n",
            "Train Progress: 0.012238%, train loss: 0.004439, norm: 1.8434\n",
            "valid loss: 0.004439\n",
            "Train Progress: 0.012402%, train loss: 0.006280, norm: 3.3958\n",
            "Train Progress: 0.012565%, train loss: 0.008191, norm: 4.0078\n",
            "Train Progress: 0.012729%, train loss: 0.005686, norm: 2.6999\n",
            "Train Progress: 0.012893%, train loss: 0.004151, norm: 2.0161\n",
            "Train Progress: 0.013056%, train loss: 0.003099, norm: 1.6001\n",
            "valid loss: 0.003099\n",
            "Train Progress: 0.013220%, train loss: 0.002926, norm: 1.3565\n",
            "Train Progress: 0.013384%, train loss: 0.002463, norm: 1.2747\n",
            "Train Progress: 0.013547%, train loss: 0.002616, norm: 1.3198\n",
            "Train Progress: 0.013711%, train loss: 0.002844, norm: 1.4268\n",
            "Train Progress: 0.013874%, train loss: 0.004243, norm: 1.5871\n",
            "valid loss: 0.004243\n",
            "Train Progress: 0.014038%, train loss: 0.005437, norm: 1.8275\n",
            "Train Progress: 0.014202%, train loss: 0.007921, norm: 3.4123\n",
            "Train Progress: 0.014365%, train loss: 0.006841, norm: 3.1897\n",
            "Train Progress: 0.014529%, train loss: 0.005462, norm: 2.1583\n",
            "Train Progress: 0.014692%, train loss: 0.003824, norm: 1.8829\n",
            "valid loss: 0.003824\n",
            "Train Progress: 0.014856%, train loss: 0.002071, norm: 1.3398\n",
            "Train Progress: 0.015020%, train loss: 0.001745, norm: 1.1537\n",
            "Train Progress: 0.015183%, train loss: 0.001870, norm: 1.4063\n",
            "Train Progress: 0.015347%, train loss: 0.002557, norm: 1.3197\n",
            "Train Progress: 0.015510%, train loss: 0.003288, norm: 1.4991\n",
            "valid loss: 0.003288\n",
            "Train Progress: 0.015674%, train loss: 0.004023, norm: 1.8794\n",
            "Train Progress: 0.015838%, train loss: 0.005665, norm: 2.1265\n",
            "Train Progress: 0.016001%, train loss: 0.003783, norm: 2.2983\n",
            "Train Progress: 0.016165%, train loss: 0.003373, norm: 2.1216\n",
            "Train Progress: 0.016329%, train loss: 0.002579, norm: 1.5542\n",
            "valid loss: 0.002579\n",
            "Train Progress: 0.016492%, train loss: 0.002442, norm: 1.2839\n",
            "Train Progress: 0.016656%, train loss: 0.002133, norm: 1.2809\n",
            "Train Progress: 0.016819%, train loss: 0.001956, norm: 1.2355\n",
            "Train Progress: 0.016983%, train loss: 0.002846, norm: 1.7147\n",
            "Train Progress: 0.017147%, train loss: 0.002867, norm: 2.2980\n",
            "valid loss: 0.002867\n",
            "Train Progress: 0.017310%, train loss: 0.005345, norm: 2.2664\n",
            "Train Progress: 0.017474%, train loss: 0.001678, norm: 1.2742\n",
            "Train Progress: 0.017637%, train loss: 0.001540, norm: 1.1422\n",
            "Train Progress: 0.017801%, train loss: 0.001742, norm: 1.0961\n",
            "Train Progress: 0.017965%, train loss: 0.001652, norm: 1.1072\n",
            "valid loss: 0.001652\n",
            "Train Progress: 0.018128%, train loss: 0.002135, norm: 1.2439\n",
            "Train Progress: 0.018292%, train loss: 0.002481, norm: 1.5500\n",
            "Train Progress: 0.018455%, train loss: 0.002672, norm: 1.9034\n",
            "Train Progress: 0.018619%, train loss: 0.006561, norm: 2.6928\n",
            "Train Progress: 0.018783%, train loss: 0.012305, norm: 2.4376\n",
            "valid loss: 0.012305\n",
            "Train Progress: 0.018946%, train loss: 0.002572, norm: 1.6225\n",
            "Train Progress: 0.019110%, train loss: 0.001837, norm: 1.3457\n",
            "Train Progress: 0.019274%, train loss: 0.002058, norm: 1.1935\n",
            "Train Progress: 0.019437%, train loss: 0.001792, norm: 1.1285\n",
            "Train Progress: 0.019601%, train loss: 0.001544, norm: 1.1351\n",
            "valid loss: 0.001544\n",
            "Train Progress: 0.019764%, train loss: 0.002435, norm: 1.3416\n",
            "Train Progress: 0.019928%, train loss: 0.002624, norm: 1.5885\n",
            "Train Progress: 0.020092%, train loss: 0.003880, norm: 2.3173\n",
            "Train Progress: 0.020255%, train loss: 0.003176, norm: 1.9933\n",
            "Train Progress: 0.020419%, train loss: 0.002236, norm: 1.9257\n",
            "valid loss: 0.002236\n",
            "Train Progress: 0.020582%, train loss: 0.001458, norm: 1.3762\n",
            "Train Progress: 0.020746%, train loss: 0.001498, norm: 1.2651\n",
            "Train Progress: 0.020910%, train loss: 0.002889, norm: 2.2657\n",
            "Train Progress: 0.021073%, train loss: 0.004681, norm: 3.0226\n",
            "Train Progress: 0.021237%, train loss: 0.004995, norm: 3.2478\n",
            "valid loss: 0.004995\n",
            "Train Progress: 0.021401%, train loss: 0.003758, norm: 2.5566\n",
            "Train Progress: 0.021564%, train loss: 0.001751, norm: 1.5191\n",
            "Train Progress: 0.021728%, train loss: 0.001352, norm: 1.1434\n",
            "Train Progress: 0.021891%, train loss: 0.001078, norm: 1.0896\n",
            "Train Progress: 0.022055%, train loss: 0.001314, norm: 1.0715\n",
            "valid loss: 0.001314\n",
            "Train Progress: 0.022219%, train loss: 0.001168, norm: 1.0806\n",
            "Train Progress: 0.022382%, train loss: 0.001772, norm: 1.0741\n",
            "Train Progress: 0.022546%, train loss: 0.005753, norm: 2.6668\n",
            "Train Progress: 0.022709%, train loss: 0.015765, norm: 8.5826\n",
            "Train Progress: 0.022873%, train loss: 0.009368, norm: 6.3484\n",
            "valid loss: 0.009368\n",
            "Train Progress: 0.023037%, train loss: 0.003606, norm: 2.1055\n",
            "Train Progress: 0.023200%, train loss: 0.003857, norm: 1.6453\n",
            "Train Progress: 0.023364%, train loss: 0.003133, norm: 1.7910\n",
            "Train Progress: 0.023527%, train loss: 0.004328, norm: 1.5384\n",
            "Train Progress: 0.023691%, train loss: 0.002009, norm: 1.3328\n",
            "valid loss: 0.002009\n",
            "Train Progress: 0.023855%, train loss: 0.001387, norm: 1.1828\n",
            "Train Progress: 0.024018%, train loss: 0.001197, norm: 1.1146\n",
            "Train Progress: 0.024182%, train loss: 0.000935, norm: 1.1075\n",
            "Train Progress: 0.024346%, train loss: 0.000892, norm: 1.0807\n",
            "Train Progress: 0.024509%, train loss: 0.002133, norm: 1.1155\n",
            "valid loss: 0.002133\n",
            "Train Progress: 0.024673%, train loss: 0.005721, norm: 4.2879\n",
            "Train Progress: 0.024836%, train loss: 0.003904, norm: 4.1051\n",
            "Train Progress: 0.025000%, train loss: 0.006484, norm: 2.5221\n",
            "Train Progress: 0.025164%, train loss: 0.003629, norm: 2.0048\n",
            "Train Progress: 0.025327%, train loss: 0.002561, norm: 1.6159\n",
            "valid loss: 0.002561\n",
            "Train Progress: 0.025491%, train loss: 0.002946, norm: 1.6736\n",
            "Train Progress: 0.025654%, train loss: 0.001850, norm: 1.3288\n",
            "Train Progress: 0.025818%, train loss: 0.002888, norm: 1.3655\n",
            "Train Progress: 0.025982%, train loss: 0.002888, norm: 1.5698\n",
            "Train Progress: 0.026145%, train loss: 0.001044, norm: 1.6480\n",
            "valid loss: 0.001044\n",
            "Train Progress: 0.026309%, train loss: 0.000857, norm: 1.1324\n",
            "Train Progress: 0.026473%, train loss: 0.002195, norm: 1.0752\n",
            "Train Progress: 0.026636%, train loss: 0.001039, norm: 1.0646\n",
            "Train Progress: 0.026800%, train loss: 0.001158, norm: 1.2803\n",
            "Train Progress: 0.026963%, train loss: 0.002436, norm: 1.3181\n",
            "valid loss: 0.002436\n",
            "Train Progress: 0.027127%, train loss: 0.001582, norm: 2.3358\n",
            "Train Progress: 0.027291%, train loss: 0.001477, norm: 1.1710\n",
            "Train Progress: 0.027454%, train loss: 0.002511, norm: 1.4556\n",
            "Train Progress: 0.027618%, train loss: 0.007163, norm: 3.9111\n",
            "Train Progress: 0.027781%, train loss: 0.004537, norm: 3.4907\n",
            "valid loss: 0.004537\n",
            "Train Progress: 0.027945%, train loss: 0.002836, norm: 1.7410\n",
            "Train Progress: 0.028109%, train loss: 0.001575, norm: 1.2344\n",
            "Train Progress: 0.028272%, train loss: 0.001354, norm: 1.2822\n",
            "Train Progress: 0.028436%, train loss: 0.001463, norm: 1.2353\n",
            "Train Progress: 0.028599%, train loss: 0.001106, norm: 1.1379\n",
            "valid loss: 0.001106\n",
            "Train Progress: 0.028763%, train loss: 0.000938, norm: 1.0908\n",
            "Train Progress: 0.028927%, train loss: 0.000944, norm: 1.0840\n",
            "Train Progress: 0.029090%, train loss: 0.001014, norm: 3.6422\n",
            "Train Progress: 0.029254%, train loss: 0.002651, norm: 1.2226\n",
            "Train Progress: 0.029418%, train loss: 0.004503, norm: 3.1858\n",
            "valid loss: 0.004503\n",
            "Train Progress: 0.029581%, train loss: 0.008076, norm: 3.8805\n",
            "Train Progress: 0.029745%, train loss: 0.009058, norm: 4.5325\n",
            "Train Progress: 0.029908%, train loss: 0.005158, norm: 2.8631\n",
            "Train Progress: 0.030072%, train loss: 0.002050, norm: 1.4984\n",
            "Train Progress: 0.030236%, train loss: 0.000991, norm: 1.2633\n",
            "valid loss: 0.000991\n",
            "Train Progress: 0.030399%, train loss: 0.000696, norm: 1.0742\n",
            "Train Progress: 0.030563%, train loss: 0.000625, norm: 1.0277\n",
            "Train Progress: 0.030726%, train loss: 0.000522, norm: 1.0745\n",
            "Train Progress: 0.030890%, train loss: 0.000653, norm: 1.0678\n",
            "Train Progress: 0.031054%, train loss: 0.000733, norm: 1.2469\n",
            "valid loss: 0.000733\n",
            "Train Progress: 0.031217%, train loss: 0.000950, norm: 1.1972\n",
            "Train Progress: 0.031381%, train loss: 0.002311, norm: 3.2206\n",
            "Train Progress: 0.031545%, train loss: 0.002879, norm: 3.2239\n",
            "Train Progress: 0.031708%, train loss: 0.004613, norm: 3.0340\n",
            "Train Progress: 0.031872%, train loss: 0.028011, norm: 5.3590\n",
            "valid loss: 0.028011\n",
            "Train Progress: 0.032035%, train loss: 0.001572, norm: 2.4748\n",
            "Train Progress: 0.032199%, train loss: 0.002230, norm: 1.3283\n",
            "Train Progress: 0.032363%, train loss: 0.002027, norm: 1.1858\n",
            "Train Progress: 0.032526%, train loss: 0.001981, norm: 1.3085\n",
            "Train Progress: 0.032690%, train loss: 0.003091, norm: 1.3761\n",
            "valid loss: 0.003091\n",
            "Train Progress: 0.032853%, train loss: 0.002121, norm: 1.3312\n",
            "Train Progress: 0.033017%, train loss: 0.001230, norm: 1.3091\n",
            "Train Progress: 0.033181%, train loss: 0.001187, norm: 1.1693\n",
            "Train Progress: 0.033344%, train loss: 0.001491, norm: 1.1038\n",
            "Train Progress: 0.033508%, train loss: 0.000955, norm: 1.1911\n",
            "valid loss: 0.000955\n",
            "Train Progress: 0.033671%, train loss: 0.000844, norm: 1.1262\n",
            "Train Progress: 0.033835%, train loss: 0.002196, norm: 1.4897\n",
            "Train Progress: 0.033999%, train loss: 0.003485, norm: 2.3712\n",
            "Train Progress: 0.034162%, train loss: 0.001097, norm: 1.8406\n",
            "Train Progress: 0.034326%, train loss: 0.000938, norm: 1.2087\n",
            "valid loss: 0.000938\n",
            "Train Progress: 0.034490%, train loss: 0.000916, norm: 1.1467\n",
            "Train Progress: 0.034653%, train loss: 0.000959, norm: 1.1406\n",
            "Train Progress: 0.034817%, train loss: 0.000827, norm: 1.0976\n",
            "Train Progress: 0.034980%, train loss: 0.001202, norm: 1.1353\n",
            "Train Progress: 0.035144%, train loss: 0.004805, norm: 3.5989\n",
            "valid loss: 0.004805\n",
            "Train Progress: 0.035308%, train loss: 0.007417, norm: 5.9966\n",
            "Train Progress: 0.035471%, train loss: 0.006655, norm: 3.1121\n",
            "Train Progress: 0.035635%, train loss: 0.002838, norm: 3.1423\n",
            "Train Progress: 0.035798%, train loss: 0.001663, norm: 1.9566\n",
            "Train Progress: 0.035962%, train loss: 0.000841, norm: 1.5487\n",
            "valid loss: 0.000841\n",
            "Train Progress: 0.036126%, train loss: 0.001040, norm: 1.1058\n",
            "Train Progress: 0.036289%, train loss: 0.000626, norm: 1.0674\n",
            "Train Progress: 0.036453%, train loss: 0.000516, norm: 1.0423\n",
            "Train Progress: 0.036616%, train loss: 0.000575, norm: 1.0516\n",
            "Train Progress: 0.036780%, train loss: 0.000647, norm: 1.0607\n",
            "valid loss: 0.000647\n",
            "Train Progress: 0.036944%, train loss: 0.000625, norm: 1.2353\n",
            "Train Progress: 0.037107%, train loss: 0.000657, norm: 1.4036\n",
            "Train Progress: 0.037271%, train loss: 0.004325, norm: 3.8043\n",
            "Train Progress: 0.037435%, train loss: 0.002196, norm: 6.0878\n",
            "Train Progress: 0.037598%, train loss: 0.001723, norm: 2.1672\n",
            "valid loss: 0.001723\n",
            "Train Progress: 0.037762%, train loss: 0.000701, norm: 1.1955\n",
            "Train Progress: 0.037925%, train loss: 0.000754, norm: 1.1660\n",
            "Train Progress: 0.038089%, train loss: 0.000970, norm: 1.1906\n",
            "Train Progress: 0.038253%, train loss: 0.001912, norm: 1.3233\n",
            "Train Progress: 0.038416%, train loss: 0.000499, norm: 1.3649\n",
            "valid loss: 0.000499\n",
            "Train Progress: 0.038580%, train loss: 0.000602, norm: 1.0274\n",
            "Train Progress: 0.038743%, train loss: 0.000546, norm: 1.0295\n",
            "Train Progress: 0.038907%, train loss: 0.000593, norm: 1.0881\n",
            "Train Progress: 0.039071%, train loss: 0.002113, norm: 1.4633\n",
            "Train Progress: 0.039234%, train loss: 0.001677, norm: 1.9326\n",
            "valid loss: 0.001677\n",
            "Train Progress: 0.039398%, train loss: 0.001556, norm: 2.1750\n",
            "Train Progress: 0.039562%, train loss: 0.002012, norm: 1.3420\n",
            "Train Progress: 0.039725%, train loss: 0.001677, norm: 1.3437\n",
            "Train Progress: 0.039889%, train loss: 0.001029, norm: 1.2513\n",
            "Train Progress: 0.040052%, train loss: 0.000994, norm: 1.1888\n",
            "valid loss: 0.000994\n",
            "Train Progress: 0.040216%, train loss: 0.000830, norm: 1.0545\n",
            "Train Progress: 0.040380%, train loss: 0.000505, norm: 1.1217\n",
            "Train Progress: 0.040543%, train loss: 0.001461, norm: 1.8760\n",
            "Train Progress: 0.040707%, train loss: 0.001002, norm: 1.6342\n",
            "Train Progress: 0.040870%, train loss: 0.000664, norm: 1.2782\n",
            "valid loss: 0.000664\n",
            "Train Progress: 0.041034%, train loss: 0.002169, norm: 1.2442\n",
            "Train Progress: 0.041198%, train loss: 0.015672, norm: 7.1220\n",
            "Train Progress: 0.041361%, train loss: 0.009627, norm: 6.4950\n",
            "Train Progress: 0.041525%, train loss: 0.004164, norm: 2.6515\n",
            "Train Progress: 0.041688%, train loss: 0.005071, norm: 2.3448\n",
            "valid loss: 0.005071\n",
            "Train Progress: 0.041852%, train loss: 0.005152, norm: 2.2420\n",
            "Train Progress: 0.042016%, train loss: 0.002207, norm: 1.4990\n",
            "Train Progress: 0.042179%, train loss: 0.001700, norm: 1.1590\n",
            "Train Progress: 0.042343%, train loss: 0.001159, norm: 1.1786\n",
            "Train Progress: 0.042507%, train loss: 0.000925, norm: 1.1233\n",
            "valid loss: 0.000925\n",
            "Train Progress: 0.042670%, train loss: 0.000982, norm: 1.0869\n",
            "Train Progress: 0.042834%, train loss: 0.000681, norm: 1.0623\n",
            "Train Progress: 0.042997%, train loss: 0.000823, norm: 1.0714\n",
            "Train Progress: 0.043161%, train loss: 0.001645, norm: 1.5288\n",
            "Train Progress: 0.043325%, train loss: 0.009358, norm: 9.8378\n",
            "valid loss: 0.009358\n",
            "Train Progress: 0.043488%, train loss: 0.022437, norm: 15.3906\n",
            "Train Progress: 0.043652%, train loss: 0.018585, norm: 10.4232\n",
            "Train Progress: 0.043815%, train loss: 0.007880, norm: 5.7328\n",
            "Train Progress: 0.043979%, train loss: 0.003110, norm: 2.1739\n",
            "Train Progress: 0.044143%, train loss: 0.004244, norm: 2.3729\n",
            "valid loss: 0.004244\n",
            "Train Progress: 0.044306%, train loss: 0.002504, norm: 2.1159\n",
            "Train Progress: 0.044470%, train loss: 0.002355, norm: 1.3308\n",
            "Train Progress: 0.044634%, train loss: 0.000638, norm: 1.0747\n",
            "Train Progress: 0.044797%, train loss: 0.000485, norm: 1.0518\n",
            "Train Progress: 0.044961%, train loss: 0.000435, norm: 1.0281\n",
            "valid loss: 0.000435\n",
            "Train Progress: 0.045124%, train loss: 0.000586, norm: 1.0431\n",
            "Train Progress: 0.045288%, train loss: 0.000820, norm: 1.1794\n",
            "Train Progress: 0.045452%, train loss: 0.000711, norm: 1.1533\n",
            "Train Progress: 0.045615%, train loss: 0.001184, norm: 1.4283\n",
            "Train Progress: 0.045779%, train loss: 0.003807, norm: 2.2084\n",
            "valid loss: 0.003807\n",
            "Train Progress: 0.045942%, train loss: 0.004710, norm: 3.9677\n",
            "Train Progress: 0.046106%, train loss: 0.012906, norm: 7.0749\n",
            "Train Progress: 0.046270%, train loss: 0.008750, norm: 6.4073\n",
            "Train Progress: 0.046433%, train loss: 0.005784, norm: 2.9433\n",
            "Train Progress: 0.046597%, train loss: 0.005110, norm: 3.7875\n",
            "valid loss: 0.005110\n",
            "Train Progress: 0.046760%, train loss: 0.001940, norm: 2.6679\n",
            "Train Progress: 0.046924%, train loss: 0.001327, norm: 1.3880\n",
            "Train Progress: 0.047088%, train loss: 0.000831, norm: 1.2931\n",
            "Train Progress: 0.047251%, train loss: 0.000612, norm: 1.1335\n",
            "Train Progress: 0.047415%, train loss: 0.000954, norm: 2.1332\n",
            "valid loss: 0.000954\n",
            "Train Progress: 0.047579%, train loss: 0.000835, norm: 1.1363\n",
            "Train Progress: 0.047742%, train loss: 0.000374, norm: 1.0248\n",
            "Train Progress: 0.047906%, train loss: 0.004291, norm: 3.9933\n",
            "Train Progress: 0.048069%, train loss: 0.014853, norm: 11.9255\n",
            "Train Progress: 0.048233%, train loss: 0.012147, norm: 8.9676\n",
            "valid loss: 0.012147\n",
            "Train Progress: 0.048397%, train loss: 0.001854, norm: 2.4256\n",
            "Train Progress: 0.048560%, train loss: 0.001512, norm: 1.5776\n",
            "Train Progress: 0.048724%, train loss: 0.001237, norm: 1.4078\n",
            "Train Progress: 0.048887%, train loss: 0.001066, norm: 1.2346\n",
            "Train Progress: 0.049051%, train loss: 0.000923, norm: 1.6135\n",
            "valid loss: 0.000923\n",
            "Train Progress: 0.049215%, train loss: 0.001377, norm: 1.9844\n",
            "Train Progress: 0.049378%, train loss: 0.001544, norm: 1.1276\n",
            "Train Progress: 0.049542%, train loss: 0.000433, norm: 1.0415\n",
            "Train Progress: 0.049705%, train loss: 0.000504, norm: 1.1302\n",
            "Train Progress: 0.049869%, train loss: 0.000509, norm: 1.0909\n",
            "valid loss: 0.000509\n",
            "Train Progress: 0.050033%, train loss: 0.000529, norm: 1.1053\n",
            "Train Progress: 0.050196%, train loss: 0.000571, norm: 1.0664\n",
            "Train Progress: 0.050360%, train loss: 0.000523, norm: 1.0877\n",
            "Train Progress: 0.050524%, train loss: 0.000392, norm: 1.0330\n",
            "Train Progress: 0.050687%, train loss: 0.000321, norm: 1.0150\n",
            "valid loss: 0.000321\n",
            "Train Progress: 0.050851%, train loss: 0.000283, norm: 1.0592\n",
            "Train Progress: 0.051014%, train loss: 0.000321, norm: 1.0646\n",
            "Train Progress: 0.051178%, train loss: 0.000543, norm: 1.0099\n",
            "Train Progress: 0.051342%, train loss: 0.003069, norm: 1.2634\n",
            "Train Progress: 0.051505%, train loss: 0.017180, norm: 5.5391\n",
            "valid loss: 0.017180\n",
            "Train Progress: 0.051669%, train loss: 0.014173, norm: 7.0040\n",
            "Train Progress: 0.051832%, train loss: 0.016111, norm: 6.9340\n",
            "Train Progress: 0.051996%, train loss: 0.009791, norm: 3.8021\n",
            "Train Progress: 0.052160%, train loss: 0.003521, norm: 2.7932\n",
            "Train Progress: 0.052323%, train loss: 0.001288, norm: 1.4099\n",
            "valid loss: 0.001288\n",
            "Train Progress: 0.052487%, train loss: 0.001123, norm: 1.1170\n",
            "Train Progress: 0.052651%, train loss: 0.000793, norm: 1.0758\n",
            "Train Progress: 0.052814%, train loss: 0.000611, norm: 2.2773\n",
            "Train Progress: 0.052978%, train loss: 0.000527, norm: 1.0120\n",
            "Train Progress: 0.053141%, train loss: 0.001708, norm: 1.6073\n",
            "valid loss: 0.001708\n",
            "Train Progress: 0.053305%, train loss: 0.003309, norm: 2.1353\n",
            "Train Progress: 0.053469%, train loss: 0.000743, norm: 1.3295\n",
            "Train Progress: 0.053632%, train loss: 0.000330, norm: 1.0776\n",
            "Train Progress: 0.053796%, train loss: 0.000268, norm: 1.0077\n",
            "Train Progress: 0.053959%, train loss: 0.000302, norm: 1.1074\n",
            "valid loss: 0.000302\n",
            "Train Progress: 0.054123%, train loss: 0.000404, norm: 1.3840\n",
            "Train Progress: 0.054287%, train loss: 0.000434, norm: 1.6818\n",
            "Train Progress: 0.054450%, train loss: 0.001735, norm: 1.2489\n",
            "Train Progress: 0.054614%, train loss: 0.005169, norm: 5.0908\n",
            "Train Progress: 0.054777%, train loss: 0.003872, norm: 5.3463\n",
            "valid loss: 0.003872\n",
            "Train Progress: 0.054941%, train loss: 0.001066, norm: 1.9548\n",
            "Train Progress: 0.055105%, train loss: 0.003781, norm: 2.2321\n",
            "Train Progress: 0.055268%, train loss: 0.003140, norm: 2.6056\n",
            "Train Progress: 0.055432%, train loss: 0.001028, norm: 1.4835\n",
            "Train Progress: 0.055596%, train loss: 0.000304, norm: 1.0512\n",
            "valid loss: 0.000304\n",
            "Train Progress: 0.055759%, train loss: 0.001326, norm: 1.0209\n",
            "Train Progress: 0.055923%, train loss: 0.000362, norm: 1.1507\n",
            "Train Progress: 0.056086%, train loss: 0.000252, norm: 1.0123\n",
            "Train Progress: 0.056250%, train loss: 0.000420, norm: 1.0130\n",
            "Train Progress: 0.056414%, train loss: 0.000295, norm: 1.0008\n",
            "valid loss: 0.000295\n",
            "Train Progress: 0.056577%, train loss: 0.000475, norm: 1.0374\n",
            "Train Progress: 0.056741%, train loss: 0.001068, norm: 1.9688\n",
            "Train Progress: 0.056904%, train loss: 0.005614, norm: 3.0980\n",
            "Train Progress: 0.057068%, train loss: 0.001945, norm: 2.2265\n",
            "Train Progress: 0.057232%, train loss: 0.078478, norm: 1.6781\n",
            "valid loss: 0.078478\n",
            "Train Progress: 0.057395%, train loss: 0.000941, norm: 1.2067\n",
            "Train Progress: 0.057559%, train loss: 0.001333, norm: 2.2682\n",
            "Train Progress: 0.057723%, train loss: 0.002832, norm: 2.2671\n",
            "Train Progress: 0.057886%, train loss: 0.000844, norm: 1.5169\n",
            "Train Progress: 0.058050%, train loss: 0.000958, norm: 1.2177\n",
            "valid loss: 0.000958\n",
            "Train Progress: 0.058213%, train loss: 0.000661, norm: 1.1385\n",
            "Train Progress: 0.058377%, train loss: 0.000731, norm: 1.0342\n",
            "Train Progress: 0.058541%, train loss: 0.000492, norm: 1.0741\n",
            "Train Progress: 0.058704%, train loss: 0.000774, norm: 1.7768\n",
            "Train Progress: 0.058868%, train loss: 0.001588, norm: 1.2628\n",
            "valid loss: 0.001588\n",
            "Train Progress: 0.059031%, train loss: 0.000846, norm: 1.2014\n",
            "Train Progress: 0.059195%, train loss: 0.002329, norm: 1.8469\n",
            "Train Progress: 0.059359%, train loss: 0.005303, norm: 4.3485\n",
            "Train Progress: 0.059522%, train loss: 0.005716, norm: 5.9381\n",
            "Train Progress: 0.059686%, train loss: 0.004511, norm: 3.9772\n",
            "valid loss: 0.004511\n",
            "Train Progress: 0.059849%, train loss: 0.001485, norm: 1.6052\n",
            "Train Progress: 0.060013%, train loss: 0.000818, norm: 1.2181\n",
            "Train Progress: 0.060177%, train loss: 0.000566, norm: 1.0916\n",
            "Train Progress: 0.060340%, train loss: 0.000826, norm: 1.1383\n",
            "Train Progress: 0.060504%, train loss: 0.000489, norm: 1.1392\n",
            "valid loss: 0.000489\n",
            "Train Progress: 0.060668%, train loss: 0.000267, norm: 1.0406\n",
            "Train Progress: 0.060831%, train loss: 0.000202, norm: 1.0078\n",
            "Train Progress: 0.060995%, train loss: 0.000160, norm: 1.0020\n",
            "Train Progress: 0.061158%, train loss: 0.000333, norm: 0.9950\n",
            "Train Progress: 0.061322%, train loss: 0.000775, norm: 2.0326\n",
            "valid loss: 0.000775\n",
            "Train Progress: 0.061486%, train loss: 0.001687, norm: 2.2122\n",
            "Train Progress: 0.061649%, train loss: 0.002101, norm: 2.2590\n",
            "Train Progress: 0.061813%, train loss: 0.004301, norm: 3.6712\n",
            "Train Progress: 0.061976%, train loss: 0.001482, norm: 2.6040\n",
            "Train Progress: 0.062140%, train loss: 0.000668, norm: 1.1013\n",
            "valid loss: 0.000668\n",
            "Train Progress: 0.062304%, train loss: 0.000595, norm: 1.3477\n",
            "Train Progress: 0.062467%, train loss: 0.001070, norm: 1.9481\n",
            "Train Progress: 0.062631%, train loss: 0.000897, norm: 1.2310\n",
            "Train Progress: 0.062795%, train loss: 0.000624, norm: 1.0896\n",
            "Train Progress: 0.062958%, train loss: 0.000231, norm: 1.0295\n",
            "valid loss: 0.000231\n",
            "Train Progress: 0.063122%, train loss: 0.000215, norm: 1.0261\n",
            "Train Progress: 0.063285%, train loss: 0.000919, norm: 1.0689\n",
            "Train Progress: 0.063449%, train loss: 0.002518, norm: 1.7710\n",
            "Train Progress: 0.063613%, train loss: 0.011300, norm: 10.5853\n",
            "Train Progress: 0.063776%, train loss: 0.074679, norm: 25.8601\n",
            "valid loss: 0.074679\n",
            "Train Progress: 0.063940%, train loss: 0.058369, norm: 29.3808\n",
            "Train Progress: 0.064103%, train loss: 0.026410, norm: 17.1907\n",
            "Train Progress: 0.064267%, train loss: 0.013429, norm: 6.3998\n",
            "Train Progress: 0.064431%, train loss: 0.001280, norm: 1.3937\n",
            "Train Progress: 0.064594%, train loss: 0.001645, norm: 1.2015\n",
            "valid loss: 0.001645\n",
            "Train Progress: 0.064758%, train loss: 0.000296, norm: 1.0225\n",
            "Train Progress: 0.064921%, train loss: 0.000338, norm: 1.1379\n",
            "Train Progress: 0.065085%, train loss: 0.000829, norm: 1.0236\n",
            "Train Progress: 0.065249%, train loss: 0.001726, norm: 1.2250\n",
            "Train Progress: 0.065412%, train loss: 0.013641, norm: 4.7129\n",
            "valid loss: 0.013641\n",
            "Train Progress: 0.065576%, train loss: 0.018540, norm: 8.4862\n",
            "Train Progress: 0.065740%, train loss: 0.013990, norm: 7.0279\n",
            "Train Progress: 0.065903%, train loss: 0.004086, norm: 2.5421\n",
            "Train Progress: 0.066067%, train loss: 0.001069, norm: 1.5176\n",
            "Train Progress: 0.066230%, train loss: 0.000666, norm: 1.2143\n",
            "valid loss: 0.000666\n",
            "Train Progress: 0.066394%, train loss: 0.000363, norm: 1.0285\n",
            "Train Progress: 0.066558%, train loss: 0.000413, norm: 1.0152\n",
            "Train Progress: 0.066721%, train loss: 0.001082, norm: 1.0319\n",
            "Train Progress: 0.066885%, train loss: 0.000481, norm: 1.0782\n",
            "Train Progress: 0.067048%, train loss: 0.000387, norm: 1.1293\n",
            "valid loss: 0.000387\n",
            "Train Progress: 0.067212%, train loss: 0.000378, norm: 1.0029\n",
            "Train Progress: 0.067376%, train loss: 0.001775, norm: 1.0064\n",
            "Train Progress: 0.067539%, train loss: 0.001558, norm: 2.0855\n",
            "Train Progress: 0.067703%, train loss: 0.008457, norm: 5.0234\n",
            "Train Progress: 0.067866%, train loss: 0.010455, norm: 6.0586\n",
            "valid loss: 0.010455\n",
            "Train Progress: 0.068030%, train loss: 0.015409, norm: 7.4253\n",
            "Train Progress: 0.068194%, train loss: 0.005788, norm: 3.7605\n",
            "Train Progress: 0.068357%, train loss: 0.002048, norm: 1.6369\n",
            "Train Progress: 0.068521%, train loss: 0.003550, norm: 1.7195\n",
            "Train Progress: 0.068685%, train loss: 0.000782, norm: 1.1807\n",
            "valid loss: 0.000782\n",
            "Train Progress: 0.068848%, train loss: 0.000371, norm: 1.0494\n",
            "Train Progress: 0.069012%, train loss: 0.000363, norm: 1.0241\n",
            "Train Progress: 0.069175%, train loss: 0.000362, norm: 1.0209\n",
            "Train Progress: 0.069339%, train loss: 0.000290, norm: 1.0270\n",
            "Train Progress: 0.069503%, train loss: 0.000212, norm: 0.9878\n",
            "valid loss: 0.000212\n",
            "Train Progress: 0.069666%, train loss: 0.008354, norm: 3.0165\n",
            "Train Progress: 0.069830%, train loss: 0.007351, norm: 7.1220\n",
            "Train Progress: 0.069993%, train loss: 0.013170, norm: 9.3430\n",
            "Train Progress: 0.070157%, train loss: 0.025766, norm: 10.9876\n",
            "Train Progress: 0.070321%, train loss: 0.007589, norm: 10.4784\n",
            "valid loss: 0.007589\n",
            "Train Progress: 0.070484%, train loss: 0.003014, norm: 2.8693\n",
            "Train Progress: 0.070648%, train loss: 0.001601, norm: 1.2686\n",
            "Train Progress: 0.070812%, train loss: 0.002848, norm: 1.7018\n",
            "Train Progress: 0.070975%, train loss: 0.001564, norm: 1.2000\n",
            "Train Progress: 0.071139%, train loss: 0.000780, norm: 1.5659\n",
            "valid loss: 0.000780\n",
            "Train Progress: 0.071302%, train loss: 0.000692, norm: 1.2593\n",
            "Train Progress: 0.071466%, train loss: 0.001383, norm: 1.4643\n",
            "Train Progress: 0.071630%, train loss: 0.000516, norm: 1.0741\n",
            "Train Progress: 0.071793%, train loss: 0.000528, norm: 1.0538\n",
            "Train Progress: 0.071957%, train loss: 0.000511, norm: 1.0490\n",
            "valid loss: 0.000511\n",
            "Train Progress: 0.072120%, train loss: 0.000425, norm: 1.0464\n",
            "Train Progress: 0.072284%, train loss: 0.000374, norm: 1.0324\n",
            "Train Progress: 0.072448%, train loss: 0.000429, norm: 1.0251\n",
            "Train Progress: 0.072611%, train loss: 0.001550, norm: 1.0434\n",
            "Train Progress: 0.072775%, train loss: 0.003871, norm: 1.7620\n",
            "valid loss: 0.003871\n",
            "Train Progress: 0.072938%, train loss: 0.022547, norm: 7.8163\n",
            "Train Progress: 0.073102%, train loss: 0.027275, norm: 10.0826\n",
            "Train Progress: 0.073266%, train loss: 0.014473, norm: 6.1541\n",
            "Train Progress: 0.073429%, train loss: 0.008221, norm: 4.6115\n",
            "Train Progress: 0.073593%, train loss: 0.004749, norm: 2.3393\n",
            "valid loss: 0.004749\n",
            "Train Progress: 0.073757%, train loss: 0.002272, norm: 1.8382\n",
            "Train Progress: 0.073920%, train loss: 0.000311, norm: 1.1386\n",
            "Train Progress: 0.074084%, train loss: 0.000195, norm: 1.0491\n",
            "Train Progress: 0.074247%, train loss: 0.000130, norm: 1.0099\n",
            "Train Progress: 0.074411%, train loss: 0.000315, norm: 1.0269\n",
            "valid loss: 0.000315\n",
            "Train Progress: 0.074575%, train loss: 0.000147, norm: 0.9988\n",
            "Train Progress: 0.074738%, train loss: 0.001092, norm: 0.9481\n",
            "Train Progress: 0.074902%, train loss: 0.003802, norm: 1.8707\n",
            "Train Progress: 0.075065%, train loss: 0.014997, norm: 9.4852\n",
            "Train Progress: 0.075229%, train loss: 0.033792, norm: 15.1517\n",
            "valid loss: 0.033792\n",
            "Train Progress: 0.075393%, train loss: 0.025277, norm: 13.0073\n",
            "Train Progress: 0.075556%, train loss: 0.005041, norm: 5.1051\n",
            "Train Progress: 0.075720%, train loss: 0.001074, norm: 1.4076\n",
            "Train Progress: 0.075884%, train loss: 0.000556, norm: 1.1090\n",
            "Train Progress: 0.076047%, train loss: 0.001331, norm: 1.4548\n",
            "valid loss: 0.001331\n",
            "Train Progress: 0.076211%, train loss: 0.000771, norm: 1.2780\n",
            "Train Progress: 0.076374%, train loss: 0.000276, norm: 1.0187\n",
            "Train Progress: 0.076538%, train loss: 0.000461, norm: 1.1160\n",
            "Train Progress: 0.076702%, train loss: 0.000594, norm: 1.1090\n",
            "Train Progress: 0.076865%, train loss: 0.000602, norm: 1.0518\n",
            "valid loss: 0.000602\n",
            "Train Progress: 0.077029%, train loss: 0.000555, norm: 1.0798\n",
            "Train Progress: 0.077192%, train loss: 0.006969, norm: 2.9533\n",
            "Train Progress: 0.077356%, train loss: 0.005784, norm: 5.1272\n",
            "Train Progress: 0.077520%, train loss: 0.002655, norm: 3.2744\n",
            "Train Progress: 0.077683%, train loss: 0.001053, norm: 1.2823\n",
            "valid loss: 0.001053\n",
            "Train Progress: 0.077847%, train loss: 0.001756, norm: 1.1499\n",
            "Train Progress: 0.078010%, train loss: 0.002180, norm: 2.6633\n",
            "Train Progress: 0.078174%, train loss: 0.004080, norm: 3.0722\n",
            "Train Progress: 0.078338%, train loss: 0.000507, norm: 1.9541\n",
            "Train Progress: 0.078501%, train loss: 0.000262, norm: 1.0306\n",
            "valid loss: 0.000262\n",
            "Train Progress: 0.078665%, train loss: 0.000242, norm: 1.0027\n",
            "Train Progress: 0.078829%, train loss: 0.000350, norm: 1.0021\n",
            "Train Progress: 0.078992%, train loss: 0.000256, norm: 1.0023\n",
            "Train Progress: 0.079156%, train loss: 0.000289, norm: 1.0250\n",
            "Train Progress: 0.079319%, train loss: 0.000277, norm: 1.1656\n",
            "valid loss: 0.000277\n",
            "Train Progress: 0.079483%, train loss: 0.001131, norm: 1.0424\n",
            "Train Progress: 0.079647%, train loss: 0.000256, norm: 1.1017\n",
            "Train Progress: 0.079810%, train loss: 0.000308, norm: 1.0387\n",
            "Train Progress: 0.079974%, train loss: 0.000283, norm: 1.0115\n",
            "Train Progress: 0.080137%, train loss: 0.000451, norm: 1.0272\n",
            "valid loss: 0.000451\n",
            "Train Progress: 0.080301%, train loss: 0.000807, norm: 1.0977\n",
            "Train Progress: 0.080465%, train loss: 0.014269, norm: 4.1712\n",
            "Train Progress: 0.080628%, train loss: 0.008229, norm: 7.3169\n",
            "Train Progress: 0.080792%, train loss: 0.004801, norm: 6.0718\n",
            "Train Progress: 0.080955%, train loss: 0.004677, norm: 2.6663\n",
            "valid loss: 0.004677\n",
            "Train Progress: 0.081119%, train loss: 0.002475, norm: 1.5535\n",
            "Train Progress: 0.081283%, train loss: 0.001040, norm: 1.1476\n",
            "Train Progress: 0.081446%, train loss: 0.001448, norm: 1.1947\n",
            "Train Progress: 0.081610%, train loss: 0.000694, norm: 1.2700\n",
            "Train Progress: 0.081774%, train loss: 0.000483, norm: 1.1296\n",
            "valid loss: 0.000483\n",
            "Train Progress: 0.081937%, train loss: 0.000496, norm: 1.0475\n",
            "Train Progress: 0.082101%, train loss: 0.000465, norm: 1.0367\n",
            "Train Progress: 0.082264%, train loss: 0.000524, norm: 1.0202\n",
            "Train Progress: 0.082428%, train loss: 0.000607, norm: 1.0643\n",
            "Train Progress: 0.082592%, train loss: 0.002055, norm: 1.4583\n",
            "valid loss: 0.002055\n",
            "Train Progress: 0.082755%, train loss: 0.001443, norm: 1.3997\n",
            "Train Progress: 0.082919%, train loss: 0.001589, norm: 1.2590\n",
            "Train Progress: 0.083082%, train loss: 0.000901, norm: 1.4024\n",
            "Train Progress: 0.083246%, train loss: 0.001590, norm: 2.0104\n",
            "Train Progress: 0.083410%, train loss: 0.003189, norm: 1.8952\n",
            "valid loss: 0.003189\n",
            "Train Progress: 0.083573%, train loss: 0.002387, norm: 2.5682\n",
            "Train Progress: 0.083737%, train loss: 0.001574, norm: 1.3422\n",
            "Train Progress: 0.083901%, train loss: 0.000569, norm: 1.1037\n",
            "Train Progress: 0.084064%, train loss: 0.000217, norm: 1.0320\n",
            "Train Progress: 0.084228%, train loss: 0.000639, norm: 1.0329\n",
            "valid loss: 0.000639\n",
            "Train Progress: 0.084391%, train loss: 0.000245, norm: 1.0080\n",
            "Train Progress: 0.084555%, train loss: 0.000333, norm: 1.0823\n",
            "Train Progress: 0.084719%, train loss: 0.006340, norm: 2.0121\n",
            "Train Progress: 0.084882%, train loss: 0.008074, norm: 2.2437\n",
            "Train Progress: 0.085046%, train loss: 0.001139, norm: 1.5832\n",
            "valid loss: 0.001139\n",
            "Train Progress: 0.085209%, train loss: 0.000341, norm: 1.0999\n",
            "Train Progress: 0.085373%, train loss: 0.000300, norm: 1.0850\n",
            "Train Progress: 0.085537%, train loss: 0.000256, norm: 1.1134\n",
            "Train Progress: 0.085700%, train loss: 0.001017, norm: 1.1692\n",
            "Train Progress: 0.085864%, train loss: 0.001293, norm: 1.5071\n",
            "valid loss: 0.001293\n",
            "Train Progress: 0.086027%, train loss: 0.000124, norm: 1.1792\n",
            "Train Progress: 0.086191%, train loss: 0.000141, norm: 1.0167\n",
            "Train Progress: 0.086355%, train loss: 0.000113, norm: 1.0059\n",
            "Train Progress: 0.086518%, train loss: 0.000115, norm: 1.0190\n",
            "Train Progress: 0.086682%, train loss: 0.000760, norm: 1.0538\n",
            "valid loss: 0.000760\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a212c88126c7>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtotal_tokens_trained\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mgrad_accum_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'model_finetune_v2.pt')"
      ],
      "metadata": {
        "id": "gxB7ZCI95S4Z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uuin5qhIeaCc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}