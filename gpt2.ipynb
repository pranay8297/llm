{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "o4SZ8dxbspE3",
        "JITNFsV0kh2-",
        "uSB-dfBNHe5m",
        "vlf5hX4cILPZ"
      ],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPMnUE63en2zw9t2xzeAOPt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranay8297/llm/blob/main/gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n",
        "!pip install triton"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MLHeQkaMCgkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dW4e5YMMEzgW"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import tiktoken\n",
        "import time\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from dataclasses import dataclass\n",
        "from einops import rearrange"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Blocks"
      ],
      "metadata": {
        "id": "Il3QX7CuvIG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50257\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available(): device = 'cuda'\n"
      ],
      "metadata": {
        "id": "V0kMNzVQ2ipV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttentionOrig(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "\n",
        "        # without Falsh Attention\n",
        "\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "        self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "\n",
        "        k = rearrange(k, 'u v (w x) -> u w v x', w = self.n_head)\n",
        "        q = rearrange(q, 'u v (w x) -> u w v x', w = self.n_head)\n",
        "        v = rearrange(v, 'u v (w x) -> u w v x', w = self.n_head)\n",
        "\n",
        "        sim = q@k.transpose(-1, -2)/math.sqrt(x.shape[-1])\n",
        "        attn = sim.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "        attn = F.softmax(attn, dim = -1)\n",
        "\n",
        "        y = attn @ v\n",
        "        y = rearrange(y, 'u w v x -> u v (w x)')\n",
        "\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class CasualSelfAttn(nn.Module):\n",
        "\n",
        "    def __init__(self, config: GPTConfig):\n",
        "\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x: torch.tensor):\n",
        "\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config: GPTConfig):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4*config.n_embd)\n",
        "        self.act = nn.GELU(approximate = 'tanh')\n",
        "        self.c_proj = nn.Linear(4*config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x: torch.tensor):\n",
        "\n",
        "        x = self.c_fc(x) # B x block_size x 4*n_embd\n",
        "        x = self.act(x) # B x block_size x 4*n_embd\n",
        "        x = self.c_proj(x) # B x block_size x n_embd\n",
        "        return x\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, config: GPTConfig):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CasualSelfAttn(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x: torch.tensor):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    '''\n",
        "    embeddings -> Casual Self Attention -> MLP\n",
        "    '''\n",
        "\n",
        "    def __init__(self, config: GPTConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "        self.transformer = nn.ModuleDict(\n",
        "            dict(\n",
        "                wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "                wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "                h = nn.ModuleList([DecoderBlock(config) for _ in range(config.n_layer)]),\n",
        "                ln_f = nn.LayerNorm(config.n_embd)\n",
        "            )\n",
        "        )\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size)\n",
        "\n",
        "        # Weight Trying Scheme\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # Initialize the model\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "\n",
        "        std = 0.02\n",
        "        if isinstance(module, nn.Linear):\n",
        "\n",
        "            if hasattr(module, \"NANOGPT_SCALE_INIT\"):\n",
        "                std *= (2*self.config.n_layer)**(-0.5)\n",
        "\n",
        "            torch.nn.init.normal_(module.weight, std = std)\n",
        "\n",
        "            if module.bias is not None: torch.nn.init.zeros_(module.bias)\n",
        "\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, std = std)\n",
        "\n",
        "    def configure_optmizers(self, lr = 6e-04, wd = 1e-01, betas = (0.9, 0.95), eps = 1e-08, device_type = device):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "          lr:\n",
        "          wd:\n",
        "          betas:\n",
        "          eps:\n",
        "          device_type:\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # divide into two param groups\n",
        "        # Assign weight decay to the ones with dim > 1\n",
        "\n",
        "        decay_params = []\n",
        "        non_decay_params = []\n",
        "        for name, param in self.named_parameters():\n",
        "            if not param.requires_grad: continue\n",
        "            if param.dim() == 1: non_decay_params.append(param)\n",
        "            else: decay_params.append(param)\n",
        "\n",
        "        fused = True if device_type == 'cuda' else False\n",
        "        optim = torch.optim.AdamW([\n",
        "                                    {'params': decay_params, \"weight_decay\": wd},\n",
        "                                    {'params': non_decay_params, \"weight_decay\": 0},\n",
        "                                  ], lr = lr, betas = betas, eps = eps, fused = fused)\n",
        "        return optim\n",
        "\n",
        "    def forward(self, idx: torch.tensor, y: torch.tensor = None):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "          idx:\n",
        "          y:\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # idx -> B, T\n",
        "        # 1. Process Embeddings\n",
        "        # 2. Iterative attention blocks\n",
        "        # 3. Pass it through lm_head for final layer\n",
        "\n",
        "        # assert idx.shape[-1] == self.config.block_size\n",
        "        positions = torch.arange(0, idx.shape[-1], step = 1).to(idx.device)\n",
        "        pos_embeddings = self.transformer.wpe(positions) # B, T\n",
        "        tok_embeddings = self.transformer.wte(idx) # B, T, C\n",
        "\n",
        "        x = pos_embeddings[None, :, :] + tok_embeddings\n",
        "\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if y is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), y.view(-1))\n",
        "\n",
        "        return  logits, loss"
      ],
      "metadata": {
        "id": "KLyRFNnf5W59"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Snippet"
      ],
      "metadata": {
        "id": "o4SZ8dxbspE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc = tiktoken.get_encoding('gpt2')\n",
        "sentence = \"Hello, I'm designed to do\"\n",
        "tokens = enc.encode(sentence)\n",
        "tokens = torch.tensor(tokens, dtype = torch.long)\n",
        "tokens = tokens.unsqueeze(0).repeat(4, 1)\n",
        "tokens = tokens.to(device)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    target_len = 30\n",
        "    x = tokens\n",
        "    for i in range(target_len):\n",
        "\n",
        "        logits = model(x)\n",
        "        logits = logits[:, -1, :]\n",
        "        topk_probs, topk_idxs = torch.topk(F.softmax(logits, dim = -1), k = 50)\n",
        "\n",
        "        ix = torch.multinomial(topk_probs, 1)\n",
        "        xcol = torch.gather(topk_idxs, -1, ix)\n",
        "        x = torch.cat([x, ix], dim = 1)\n",
        "\n",
        "for i in x:\n",
        "    decoded = enc.decode(i.tolist())\n",
        "    print(decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zY2kEtJhso0d",
        "outputId": "e02f324b-5aee-4176-d541-0d5202e063c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-18292a81c9a6>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtarget_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Set and Data Loader"
      ],
      "metadata": {
        "id": "JITNFsV0kh2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/build-nanogpt/master/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGS7cySTkhUj",
        "outputId": "ee75acfd-ef7c-4a72-b52a-77dd74b33d2f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-21 01:09:25--  https://raw.githubusercontent.com/karpathy/build-nanogpt/master/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.3’\n",
            "\n",
            "input.txt.3         100%[===================>]   1.06M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-09-21 01:09:25 (84.7 MB/s) - ‘input.txt.3’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoaderLite:\n",
        "    def __init__(self, B: int, T: int = 1024):\n",
        "\n",
        "        with open('./input.txt', 'r') as f: data = f.read()\n",
        "        tokens = enc.encode(data)\n",
        "        self.tokens = torch.tensor(tokens, dtype = torch.long)\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "        self.current_position = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        if self.current_position + self.B*self.T > len(self.tokens):\n",
        "            self.current_position = 0\n",
        "\n",
        "        offset = self.current_position + self.B*self.T + 1\n",
        "        if offset > len(self.tokens): offset = len(tokens)\n",
        "\n",
        "        buf = self.tokens[self.current_position:offset]\n",
        "        x = buf[:-1].view(self.B, -1)\n",
        "        y = buf[1:].view(self.B, -1)\n",
        "\n",
        "        self.current_position += self.B*self.T\n",
        "        if self.current_position > len(self.tokens): self.current_position = 0\n",
        "\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "eD_bQnAxhK6I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Activations"
      ],
      "metadata": {
        "id": "uSB-dfBNHe5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPTConfig()\n",
        "model = GPT(config)\n",
        "model = model.to(device)\n",
        "dl = DataLoaderLite(B = 4, T = 64)"
      ],
      "metadata": {
        "id": "Am45sr62HokA"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = dl.next_batch()\n",
        "idx = x\n",
        "positions = torch.arange(0, idx.shape[-1], step = 1).to(idx.device)\n",
        "pos_embeddings = model.transformer.wpe(positions) # B, T\n",
        "tok_embeddings = model.transformer.wte(idx) # B, T, C\n",
        "\n",
        "x = pos_embeddings[None, :, :] + tok_embeddings\n",
        "print(x.mean(), x.std())\n",
        "for block in model.transformer.h:\n",
        "    x = block(x)\n",
        "    print(x.mean(), x.std())\n",
        "\n",
        "x = model.transformer.ln_f(x)\n",
        "print(x.mean(), x.std())\n",
        "logits = model.lm_head(x)\n",
        "print(x.mean(), x.std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHGmY6RAHfca",
        "outputId": "9b08abf8-3f96-45f0-b3b3-6b14fc8d780b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-0.0002, grad_fn=<MeanBackward0>) tensor(0.0283, grad_fn=<StdBackward0>)\n",
            "tensor(-0.0011, grad_fn=<MeanBackward0>) tensor(0.0807, grad_fn=<StdBackward0>)\n",
            "tensor(-0.0015, grad_fn=<MeanBackward0>) tensor(0.1140, grad_fn=<StdBackward0>)\n",
            "tensor(-0.0006, grad_fn=<MeanBackward0>) tensor(0.1441, grad_fn=<StdBackward0>)\n",
            "tensor(-0.0006, grad_fn=<MeanBackward0>) tensor(0.1683, grad_fn=<StdBackward0>)\n",
            "tensor(0.0013, grad_fn=<MeanBackward0>) tensor(0.1912, grad_fn=<StdBackward0>)\n",
            "tensor(0.0025, grad_fn=<MeanBackward0>) tensor(0.2076, grad_fn=<StdBackward0>)\n",
            "tensor(0.0042, grad_fn=<MeanBackward0>) tensor(0.2256, grad_fn=<StdBackward0>)\n",
            "tensor(0.0050, grad_fn=<MeanBackward0>) tensor(0.2431, grad_fn=<StdBackward0>)\n",
            "tensor(0.0014, grad_fn=<MeanBackward0>) tensor(0.2584, grad_fn=<StdBackward0>)\n",
            "tensor(0.0015, grad_fn=<MeanBackward0>) tensor(0.2763, grad_fn=<StdBackward0>)\n",
            "tensor(-0.0010, grad_fn=<MeanBackward0>) tensor(0.2919, grad_fn=<StdBackward0>)\n",
            "tensor(0.0005, grad_fn=<MeanBackward0>) tensor(0.3065, grad_fn=<StdBackward0>)\n",
            "tensor(2.3283e-10, grad_fn=<MeanBackward0>) tensor(0.9999, grad_fn=<StdBackward0>)\n",
            "tensor(2.3283e-10, grad_fn=<MeanBackward0>) tensor(0.9999, grad_fn=<StdBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Andrej's Code"
      ],
      "metadata": {
        "id": "vlf5hX4cILPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# just for reference - Do not run.\n",
        "\n",
        "import sys\n",
        "# sys.exit(0)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
        "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024 # max sequence length\n",
        "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
        "    n_layer: int = 12 # number of layers\n",
        "    n_head: int = 12 # number of heads\n",
        "    n_embd: int = 768 # embedding dimension\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing scheme\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # init params\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is of shape (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        # forward the token and posisition embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n"
      ],
      "metadata": {
        "id": "4m7vxm3UILCu"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPTConfig()\n",
        "model = GPT(config)\n",
        "model = model.to(device)\n",
        "# dl = DataLoaderLite(B = 4)"
      ],
      "metadata": {
        "id": "Av4Uh7plJHxe"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = dl.next_batch()\n",
        "idx = x\n",
        "positions = torch.arange(0, idx.shape[-1], step = 1).to(idx.device)\n",
        "pos_embeddings = model.transformer.wpe(positions) # B, T\n",
        "tok_embeddings = model.transformer.wte(idx) # B, T, C\n",
        "\n",
        "x = pos_embeddings[None, :, :] + tok_embeddings\n",
        "print(x.mean(), x.std())\n",
        "for block in model.transformer.h:\n",
        "    x = block(x)\n",
        "    print(x.mean(), x.std())\n",
        "\n",
        "x = model.transformer.ln_f(x)\n",
        "print(x.mean(), x.std())\n",
        "logits = model.lm_head(x)\n",
        "print(x.mean(), x.std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtfRh-OOJDTt",
        "outputId": "a656423b-5b5d-4da9-82eb-69188bcd0fea"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3.7030e-05, grad_fn=<MeanBackward0>) tensor(0.0283, grad_fn=<StdBackward0>)\n",
            "tensor(0.0010, grad_fn=<MeanBackward0>) tensor(0.0800, grad_fn=<StdBackward0>)\n",
            "tensor(0.0003, grad_fn=<MeanBackward0>) tensor(0.1148, grad_fn=<StdBackward0>)\n",
            "tensor(-0.0019, grad_fn=<MeanBackward0>) tensor(0.1408, grad_fn=<StdBackward0>)\n",
            "tensor(-0.0028, grad_fn=<MeanBackward0>) tensor(0.1654, grad_fn=<StdBackward0>)\n",
            "tensor(-0.0051, grad_fn=<MeanBackward0>) tensor(0.1873, grad_fn=<StdBackward0>)\n",
            "tensor(-0.0071, grad_fn=<MeanBackward0>) tensor(0.2055, grad_fn=<StdBackward0>)\n",
            "tensor(-0.0104, grad_fn=<MeanBackward0>) tensor(0.2224, grad_fn=<StdBackward0>)\n",
            "tensor(-0.0106, grad_fn=<MeanBackward0>) tensor(0.2385, grad_fn=<StdBackward0>)\n",
            "tensor(-0.0122, grad_fn=<MeanBackward0>) tensor(0.2589, grad_fn=<StdBackward0>)\n",
            "tensor(-0.0141, grad_fn=<MeanBackward0>) tensor(0.2735, grad_fn=<StdBackward0>)\n",
            "tensor(-0.0122, grad_fn=<MeanBackward0>) tensor(0.2882, grad_fn=<StdBackward0>)\n",
            "tensor(-0.0116, grad_fn=<MeanBackward0>) tensor(0.2997, grad_fn=<StdBackward0>)\n",
            "tensor(9.3132e-10, grad_fn=<MeanBackward0>) tensor(0.9999, grad_fn=<StdBackward0>)\n",
            "tensor(9.3132e-10, grad_fn=<MeanBackward0>) tensor(0.9999, grad_fn=<StdBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Play Ground"
      ],
      "metadata": {
        "id": "YzuEp_mQvMXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if device == 'cuda':\n",
        "    torch.set_float32_matmul_precision('high')"
      ],
      "metadata": {
        "id": "VEGb6dcf1j_d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPTConfig(vocab_size = 50304)\n",
        "model = GPT(config)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "Ibf9bvvbSfdb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample run on a dataset\n",
        "# simple training loop for just one batch\n",
        "iterations = 50\n",
        "tokens_per_grad_update = 2**19\n",
        "\n",
        "B = 16; T = 1024\n",
        "assert tokens_per_grad_update % (B*T) == 0\n",
        "grad_accumulation_steps = int(tokens_per_grad_update/(B*T))\n",
        "print(f\"Gradiant Accumulation Steps: {grad_accumulation_steps}\")\n",
        "\n",
        "# opt = torch.optim.AdamW(model.parameters(), lr = 6e-04, betas = (0.9, 0.95), eps = 1e-08)\n",
        "\n",
        "if device == 'cuda': model = torch.compile(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNeE0IPYD4i7",
        "outputId": "c45c7af6-8bb4-4136-873e-9d9b91954a0b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradiant Accumulation Steps: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "opt = model.configure_optmizers(lr = 6e-04, wd = 1e-01, betas = (0.9, 0.95), eps = 1e-08, device_type = device)\n",
        "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr = 6e-04, total_steps = iterations, final_div_factor=10.0)\n",
        "dl = DataLoaderLite(B = B, T = T)\n",
        "\n",
        "for i in range(iterations):\n",
        "    opt.zero_grad()\n",
        "    accumulated_loss = 0.\n",
        "\n",
        "    t1 = time.time()\n",
        "    for j in range(grad_accumulation_steps):\n",
        "        x, y = dl.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        if device == 'cuda':\n",
        "            with torch.autocast(device_type = device, dtype = torch.bfloat16):\n",
        "                logits, loss = model(x, y)\n",
        "        else:\n",
        "            logits, loss = model(x, y)\n",
        "\n",
        "        loss /= grad_accumulation_steps\n",
        "        accumulated_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "    if device == 'cuda': torch.cuda.synchronize()\n",
        "    t2 = time.time()\n",
        "    elapsed_time = t2 - t1\n",
        "    tps = B*T*grad_accumulation_steps/elapsed_time\n",
        "\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    opt.step()\n",
        "    lr_scheduler.step()\n",
        "    print(f'Iteration: {i} | Loss: {accumulated_loss:.5f} | norm: {norm:.5f} | time: {(elapsed_time/grad_accumulation_steps):.4f} | tps: {tps:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SOxx9VWgovF",
        "outputId": "0c119ca4-2174-4f31-d014-bff5dbbbeaba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0 | Loss: 6.05822 | norm: 0.50192 | time: 0.4632 | tps: 35374.812\n",
            "Iteration: 1 | Loss: 6.04270 | norm: 0.60625 | time: 0.4650 | tps: 35231.252\n",
            "Iteration: 2 | Loss: 6.04704 | norm: 0.57237 | time: 0.4582 | tps: 35758.706\n",
            "Iteration: 3 | Loss: 6.02490 | norm: 0.90870 | time: 0.4522 | tps: 36233.069\n",
            "Iteration: 4 | Loss: 6.15856 | norm: 2.79147 | time: 0.4486 | tps: 36522.788\n",
            "Iteration: 5 | Loss: 6.01018 | norm: 1.31635 | time: 0.4486 | tps: 36522.172\n",
            "Iteration: 6 | Loss: 6.04222 | norm: 2.59953 | time: 0.4528 | tps: 36181.156\n",
            "Iteration: 7 | Loss: 6.14822 | norm: 2.43822 | time: 0.4555 | tps: 35967.000\n",
            "Iteration: 8 | Loss: 6.03713 | norm: 1.77703 | time: 0.4558 | tps: 35947.154\n",
            "Iteration: 9 | Loss: 5.97217 | norm: 1.76285 | time: 0.4558 | tps: 35948.685\n",
            "Iteration: 10 | Loss: 5.95799 | norm: 1.78912 | time: 0.4538 | tps: 36107.264\n",
            "Iteration: 11 | Loss: 5.90646 | norm: 0.90164 | time: 0.4529 | tps: 36172.805\n",
            "Iteration: 12 | Loss: 5.90789 | norm: 0.81700 | time: 0.4527 | tps: 36192.382\n",
            "Iteration: 13 | Loss: 5.87500 | norm: 0.76322 | time: 0.4537 | tps: 36111.949\n",
            "Iteration: 14 | Loss: 5.85339 | norm: 0.66755 | time: 0.4563 | tps: 35909.607\n",
            "Iteration: 15 | Loss: 5.84162 | norm: 0.50257 | time: 0.4571 | tps: 35846.471\n",
            "Iteration: 16 | Loss: 5.81361 | norm: 0.52843 | time: 0.4566 | tps: 35880.239\n",
            "Iteration: 17 | Loss: 5.83242 | norm: 0.90559 | time: 0.4554 | tps: 35976.037\n",
            "Iteration: 18 | Loss: 5.79083 | norm: 0.81477 | time: 0.4546 | tps: 36039.166\n",
            "Iteration: 19 | Loss: 5.77434 | norm: 0.51962 | time: 0.4549 | tps: 36019.174\n",
            "Iteration: 20 | Loss: 5.77507 | norm: 0.36867 | time: 0.4543 | tps: 36065.570\n",
            "Iteration: 21 | Loss: 5.75172 | norm: 0.33287 | time: 0.4546 | tps: 36037.939\n",
            "Iteration: 22 | Loss: 5.77715 | norm: 0.34296 | time: 0.4557 | tps: 35950.809\n",
            "Iteration: 23 | Loss: 5.75041 | norm: 0.33657 | time: 0.4572 | tps: 35832.683\n",
            "Iteration: 24 | Loss: 5.75119 | norm: 0.33513 | time: 0.4575 | tps: 35808.545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Without torch.compile\n",
        "# Sample run on a dataset\n",
        "# simple training loop for just one batch\n",
        "config = GPTConfig(vocab_size = 50304)\n",
        "model = GPT(config)\n",
        "model = model.to(device)\n",
        "\n",
        "iterations = 50\n",
        "tokens_per_grad_update = 2**19\n",
        "\n",
        "B = 16; T = 1024\n",
        "assert tokens_per_grad_update % (B*T) == 0\n",
        "grad_accumulation_steps = int(tokens_per_grad_update/(B*T))\n",
        "print(f\"Gradiant Accumulation Steps: {grad_accumulation_steps}\")\n",
        "\n",
        "# opt = torch.optim.AdamW(model.parameters(), lr = 6e-04, betas = (0.9, 0.95), eps = 1e-08)\n",
        "\n",
        "# if device == 'cuda': model = torch.compile(model)\n",
        "\n",
        "opt = model.configure_optmizers(lr = 6e-04, wd = 1e-01, betas = (0.9, 0.95), eps = 1e-08, device_type = device)\n",
        "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr = 6e-04, total_steps = iterations, final_div_factor=10.0)\n",
        "dl = DataLoaderLite(B = B, T = T)\n",
        "\n",
        "for i in range(iterations):\n",
        "    opt.zero_grad()\n",
        "    accumulated_loss = 0.\n",
        "\n",
        "    t1 = time.time()\n",
        "    for j in range(grad_accumulation_steps):\n",
        "        x, y = dl.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        if device == 'cuda':\n",
        "            with torch.autocast(device_type = device, dtype = torch.bfloat16):\n",
        "                logits, loss = model(x, y)\n",
        "        else:\n",
        "            logits, loss = model(x, y)\n",
        "\n",
        "        loss /= grad_accumulation_steps\n",
        "        accumulated_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "    if device == 'cuda': torch.cuda.synchronize()\n",
        "    t2 = time.time()\n",
        "    elapsed_time = t2 - t1\n",
        "    tps = B*T*grad_accumulation_steps/elapsed_time\n",
        "\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    opt.step()\n",
        "    lr_scheduler.step()\n",
        "    print(f'Iteration: {i} | Loss: {accumulated_loss:.5f} | norm: {norm:.5f} | time: {(elapsed_time/grad_accumulation_steps):.4f} | tps: {tps:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIsYArXtHk4V",
        "outputId": "2c1279aa-cbc8-4757-e862-2c82242a5d5a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradiant Accumulation Steps: 32\n",
            "Iteration: 0 | Loss: 10.95100 | norm: 25.79529 | time: 0.5580 | tps: 29360.654\n",
            "Iteration: 1 | Loss: 9.78726 | norm: 11.57891 | time: 0.5275 | tps: 31059.412\n",
            "Iteration: 2 | Loss: 9.34380 | norm: 7.52412 | time: 0.5224 | tps: 31361.968\n",
            "Iteration: 3 | Loss: 9.36433 | norm: 11.63742 | time: 0.5188 | tps: 31578.372\n",
            "Iteration: 4 | Loss: 8.97734 | norm: 3.80904 | time: 0.5195 | tps: 31538.441\n",
            "Iteration: 5 | Loss: 9.09808 | norm: 10.48295 | time: 0.5202 | tps: 31497.045\n",
            "Iteration: 6 | Loss: 8.76523 | norm: 3.29494 | time: 0.5196 | tps: 31533.860\n",
            "Iteration: 7 | Loss: 8.58056 | norm: 2.82115 | time: 0.5202 | tps: 31495.291\n",
            "Iteration: 8 | Loss: 8.31629 | norm: 2.22433 | time: 0.5199 | tps: 31510.989\n",
            "Iteration: 9 | Loss: 8.07400 | norm: 3.08333 | time: 0.5194 | tps: 31541.356\n",
            "Iteration: 10 | Loss: 7.78656 | norm: 2.92010 | time: 0.5196 | tps: 31534.794\n",
            "Iteration: 11 | Loss: 7.46839 | norm: 2.42303 | time: 0.5195 | tps: 31539.955\n",
            "Iteration: 12 | Loss: 7.15307 | norm: 1.96643 | time: 0.5194 | tps: 31543.353\n",
            "Iteration: 13 | Loss: 6.82177 | norm: 1.33495 | time: 0.5202 | tps: 31496.341\n",
            "Iteration: 14 | Loss: 6.54441 | norm: 1.40472 | time: 0.5195 | tps: 31540.136\n",
            "Iteration: 15 | Loss: 6.37059 | norm: 1.11007 | time: 0.5193 | tps: 31547.786\n",
            "Iteration: 16 | Loss: 6.23207 | norm: 1.21822 | time: 0.5202 | tps: 31497.533\n",
            "Iteration: 17 | Loss: 6.14805 | norm: 1.20073 | time: 0.5195 | tps: 31536.601\n",
            "Iteration: 18 | Loss: 6.25161 | norm: 5.73466 | time: 0.5194 | tps: 31542.384\n",
            "Iteration: 19 | Loss: 6.03369 | norm: 1.18687 | time: 0.5195 | tps: 31539.003\n",
            "Iteration: 20 | Loss: 6.09897 | norm: 4.41173 | time: 0.5213 | tps: 31428.216\n",
            "Iteration: 21 | Loss: 6.01933 | norm: 2.78374 | time: 0.5195 | tps: 31539.898\n",
            "Iteration: 22 | Loss: 5.99827 | norm: 1.46353 | time: 0.5194 | tps: 31541.672\n",
            "Iteration: 23 | Loss: 5.98135 | norm: 1.88111 | time: 0.5195 | tps: 31540.702\n",
            "Iteration: 24 | Loss: 5.92736 | norm: 1.32562 | time: 0.5195 | tps: 31539.738\n",
            "Iteration: 25 | Loss: 5.89038 | norm: 0.92879 | time: 0.5195 | tps: 31540.991\n",
            "Iteration: 26 | Loss: 5.86133 | norm: 1.16336 | time: 0.5195 | tps: 31540.486\n",
            "Iteration: 27 | Loss: 5.84300 | norm: 0.64450 | time: 0.5207 | tps: 31465.535\n",
            "Iteration: 28 | Loss: 5.82287 | norm: 1.52797 | time: 0.5195 | tps: 31537.917\n",
            "Iteration: 29 | Loss: 5.77976 | norm: 0.90647 | time: 0.5195 | tps: 31537.884\n",
            "Iteration: 30 | Loss: 5.76953 | norm: 0.87525 | time: 0.5195 | tps: 31538.249\n",
            "Iteration: 31 | Loss: 5.73554 | norm: 1.08505 | time: 0.5201 | tps: 31503.126\n",
            "Iteration: 32 | Loss: 5.73184 | norm: 0.77563 | time: 0.5195 | tps: 31539.495\n",
            "Iteration: 33 | Loss: 5.71238 | norm: 2.17301 | time: 0.5201 | tps: 31499.989\n",
            "Iteration: 34 | Loss: 5.68123 | norm: 0.69954 | time: 0.5194 | tps: 31541.723\n",
            "Iteration: 35 | Loss: 5.68184 | norm: 0.82182 | time: 0.5195 | tps: 31539.627\n",
            "Iteration: 36 | Loss: 5.65398 | norm: 0.71381 | time: 0.5195 | tps: 31540.810\n",
            "Iteration: 37 | Loss: 5.65711 | norm: 0.44812 | time: 0.5195 | tps: 31537.257\n",
            "Iteration: 38 | Loss: 5.62517 | norm: 0.67951 | time: 0.5208 | tps: 31461.554\n",
            "Iteration: 39 | Loss: 5.61823 | norm: 0.64405 | time: 0.5207 | tps: 31463.993\n",
            "Iteration: 40 | Loss: 5.61011 | norm: 0.49246 | time: 0.5195 | tps: 31537.830\n",
            "Iteration: 41 | Loss: 5.58902 | norm: 0.43349 | time: 0.5219 | tps: 31392.032\n",
            "Iteration: 42 | Loss: 5.61031 | norm: 0.40386 | time: 0.5202 | tps: 31498.259\n",
            "Iteration: 43 | Loss: 5.57951 | norm: 0.42768 | time: 0.5207 | tps: 31462.336\n",
            "Iteration: 44 | Loss: 5.58268 | norm: 0.47270 | time: 0.5213 | tps: 31430.049\n",
            "Iteration: 45 | Loss: 5.58231 | norm: 0.43930 | time: 0.5201 | tps: 31503.525\n",
            "Iteration: 46 | Loss: 5.56779 | norm: 0.44451 | time: 0.5194 | tps: 31541.391\n",
            "Iteration: 47 | Loss: 5.59288 | norm: 0.38662 | time: 0.5195 | tps: 31538.067\n",
            "Iteration: 48 | Loss: 5.56404 | norm: 0.36711 | time: 0.5202 | tps: 31497.352\n",
            "Iteration: 49 | Loss: 5.57063 | norm: 0.35944 | time: 0.5213 | tps: 31427.218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = x[:, :1000]"
      ],
      "metadata": {
        "id": "l1BMRYMSL2MB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, './initial_run.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "GEUomVnAMfK5",
        "outputId": "51f9a894-6d20-4938-93cc-df49460f3aad"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-d419dc51e51f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./initial_run.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_disable_byteorder_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;31m# .cpu() on the underlying Storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/storage.py\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;34m\"\"\"Return a CPU copy of this storage if it's not already on the CPU.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUntypedStorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    target_len = 5\n",
        "    for i in range(target_len):\n",
        "        print(i)\n",
        "        logits, _ = model(x)\n",
        "        logits = logits[:, -1, :]\n",
        "        topk_probs, topk_idxs = torch.topk(F.softmax(logits, dim = -1), k = 50)\n",
        "\n",
        "        ix = torch.multinomial(topk_probs, 1)\n",
        "        xcol = torch.gather(topk_idxs, -1, ix)\n",
        "        x = torch.cat([x, ix], dim = 1)\n",
        "\n",
        "for i in x:\n",
        "    decoded = enc.decode(i.tolist())\n",
        "    print(decoded[-6:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "j0RBQTPXkB6b",
        "outputId": "bda9c35a-f598-4961-fe4f-d9024322d6a2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-b164a700c24c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtopk_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-44b2c3cca38e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, y)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;31m# assert idx.shape[-1] == self.config.block_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mpositions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mpos_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# B, T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mtok_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# B, T, C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape, x.device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4QXGKw9MANY",
        "outputId": "3818e6f7-7260-451e-8af7-0c66082ad6c4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([16, 1000]), device(type='cuda', index=0))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = None\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "sVvujwPZ_5NP"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}